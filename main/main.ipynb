{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "from text import _clean_text\n",
    "from text import _symbol_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import tgt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchaudio as TA\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_config = yaml.load(open('../config/preprocessing.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "model_config = yaml.load(open('../config/model.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "raw_path = preprocessing_config['path']['raw_path']\n",
    "input_dir = preprocessing_config['path']['input_dir']\n",
    "\n",
    "sampling_rate = preprocessing_config['audio']['sampling_rate']\n",
    "hop_length = preprocessing_config['audio']['hop_length']\n",
    "win_length = preprocessing_config['audio']['win_length']\n",
    "n_fft = preprocessing_config['audio']['n_fft']\n",
    "n_mels = preprocessing_config['audio']['n_mel_channels']\n",
    "fmin = preprocessing_config['audio']['mel_fmin']\n",
    "fmax = preprocessing_config['audio']['mel_fmax']\n",
    "cleaners = preprocessing_config['text']['text_cleaners']\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(raw_path, 'metadata.csv'), sep='|', header=None)\n",
    "metadata.columns = ['file', 'text', 'text_']\n",
    "metadata.drop(['text_'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input dir and copy files over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../data/raw_data'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = './../kaggle_data/raw_data'\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    os.makedirs(input_dir)\n",
    "\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../data/raw_data'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13100it [07:58, 27.38it/s]\n"
     ]
    }
   ],
   "source": [
    "silent_phones = [\"sil\", \"sp\", \"spn\"]\n",
    "mel_spec_transform = MelSpectrogram(sample_rate=sampling_rate, n_fft=n_fft, win_length=win_length, hop_length=hop_length, f_min=fmin, f_max=fmax, n_mels=n_mels)\n",
    "\n",
    "for line in tqdm(metadata.iterrows()):\n",
    "    file = line[1]['file']\n",
    "    text = line[1]['text']\n",
    "    clean_text = _clean_text(text, cleaner_names=cleaners)\n",
    "    \n",
    "    textgrid = tgt.io.read_textgrid(os.path.join(raw_path, 'TextGrid', 'LJSpeech', f'{file}.TextGrid'))\n",
    "    textgrid = textgrid.get_tier_by_name('phones')\n",
    "\n",
    "    phonemes = []\n",
    "    durations = []\n",
    "\n",
    "    start_time = 0.0\n",
    "    end_time = 0.0\n",
    "    end_idx = 0\n",
    "    \n",
    "    for tier in textgrid._objects:\n",
    "\n",
    "        # trim the initial silent phonemes\n",
    "        if tier.text in silent_phones and len(durations) == 0:\n",
    "            continue\n",
    "\n",
    "        # record the start time\n",
    "        if len(phonemes) == 0:\n",
    "            start_time = tier.start_time\n",
    "\n",
    "        phonemes.append(tier.text)\n",
    "        durations.append(round( tier.end_time * sampling_rate / hop_length ) - round( tier.start_time * sampling_rate / hop_length ))\n",
    "\n",
    "        # trim the last silent phonemes\n",
    "        if tier.text not in silent_phones:\n",
    "            end_time = tier.end_time\n",
    "            end_idx = len(phonemes)\n",
    "\n",
    "       \n",
    "    # compute the mel-spectogram\n",
    "    phonemes = phonemes[:end_idx]\n",
    "    durations = durations[:end_idx]\n",
    "    phoneme_seq = \"{\" + \" \".join(phonemes) + \"}\"\n",
    "\n",
    "    wav_path = os.path.join(raw_path, 'wavs', f'{file}.wav')\n",
    "    audio, _ = librosa.load(wav_path, sr=sampling_rate)\n",
    "    audio = torch.clip(torch.FloatTensor(audio).unsqueeze(0), -1, 1)\n",
    "    mel = mel_spec_transform(audio).squeeze(0)[:, :sum(durations)]\n",
    "\n",
    "    assert mel.shape[-1] == np.sum(durations), f\"Number of frames not equal for file {line[0]}\"\n",
    "\n",
    "    sample_input_dir = os.path.join(input_dir, \"LJSpeech\", file)\n",
    "    audio_file_name = os.path.join(sample_input_dir, f'{file}.wav')\n",
    "    mel_file_name = os.path.join(sample_input_dir, f'{file}-mel.npy')\n",
    "    duration_file_name = os.path.join(sample_input_dir, f'{file}-duration.npy')\n",
    "    text_file_name = os.path.join(sample_input_dir, f'{file}.csv')\n",
    "\n",
    "    if not os.path.exists(sample_input_dir):\n",
    "        os.makedirs(sample_input_dir)\n",
    "\n",
    "    wavfile.write(audio_file_name, sampling_rate, audio.reshape(-1, 1).numpy())\n",
    "    np.save(mel_file_name, mel.T)\n",
    "    np.save(duration_file_name, durations)\n",
    "    np.save(mel_file_name, mel.T)\n",
    "\n",
    "    with open(text_file_name, 'w') as f:\n",
    "        f.write(f'{file[0]} | {clean_text} | {phoneme_seq}')\n",
    "\n",
    "    sample_temp_dir = os.path.join(temp_dir, \"LJSpeech\", file)\n",
    "    audio_file_name = os.path.join(sample_temp_dir, f'{file}.wav')\n",
    "    mel_file_name = os.path.join(sample_temp_dir, f'{file}-mel.npy')\n",
    "    duration_file_name = os.path.join(sample_temp_dir, f'{file}-duration.npy')\n",
    "    text_file_name = os.path.join(sample_temp_dir, f'{file}.csv')\n",
    "\n",
    "    if not os.path.exists(sample_temp_dir):\n",
    "        os.makedirs(sample_temp_dir)\n",
    "\n",
    "    #wavfile.write(audio_file_name, sampling_rate, audio.reshape(-1, 1).numpy())\n",
    "    np.save(mel_file_name, mel.T)\n",
    "    np.save(duration_file_name, durations)\n",
    "    np.save(mel_file_name, mel.T)\n",
    "\n",
    "    with open(text_file_name, 'w') as f:\n",
    "        f.write(f'{file[0]} | {clean_text} | {phoneme_seq}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LJSpeechDataset(Dataset):\n",
    "    def __init__(self,  input_dir, input_file, max_src_len, max_trg_len, split='train', test_batch=False):\n",
    "        metadata = pd.read_csv(input_file, sep='|', header=None)\n",
    "        metadata.columns = ['file', 'text', 'text_']\n",
    "\n",
    "        self.input_dir = input_dir\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_trg_len = max_trg_len\n",
    "        \n",
    "        file_names = metadata['file'].to_numpy()\n",
    "\n",
    "        if test_batch:\n",
    "            self.file_names = file_names[:16]\n",
    "            return\n",
    "\n",
    "        x_train, x_test = train_test_split(file_names, test_size=0.2, random_state=42, shuffle=True)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.file_names = x_train\n",
    "        else:\n",
    "            self.file_names = x_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        mel = np.load(os.path.join(self.input_dir, file_name, f'{file_name}-mel.npy'))\n",
    "        duration = np.load(os.path.join(self.input_dir, file_name, f'{file_name}-duration.npy'))\n",
    "        phones = pd.read_csv(os.path.join(self.input_dir, file_name, f'{file_name}.csv'), sep='|', header=None).iloc[0][2].replace('{', '').replace('}', '').strip().split(' ')\n",
    "        phone_mapping = torch.tensor([ _symbol_to_id[symbol] for symbol in phones ])\n",
    "        \n",
    "\n",
    "        src_len = torch.tensor(len(phones))\n",
    "        trg_len = torch.tensor(mel.shape[0])\n",
    "\n",
    "        phoneme_pad_length = self.max_src_len - src_len\n",
    "        mel_pad_length = self.max_trg_len - trg_len\n",
    "        \n",
    "        phone_mapping = F.pad(phone_mapping, (0, phoneme_pad_length), mode='constant', value=0)\n",
    "        duration = F.pad( torch.tensor(duration), (0, phoneme_pad_length), mode='constant', value=0)\n",
    "        mel = F.pad(torch.tensor(mel), (0, 0, 0, mel_pad_length), mode='constant', value=0)\n",
    "        \n",
    "        return  src_len, trg_len, duration, phone_mapping, mel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, split='train')\n",
    "val_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, split='test')\n",
    "\n",
    "# set shuffle to false since we already shuffle when the data is split into train/test\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=4)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads=8, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.embed_dim = embed_dim \n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        assert self.head_dim * self.n_heads == self.embed_dim, \"The embedding dimension must be divisible by the number of heads\"\n",
    "\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(self.head_dim * self.n_heads, self.embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query_dim -> B, num_phonemes, embed_dim -> B, num_phonemes, n_heads, head_dim\n",
    "        B = query.shape[0]\n",
    "        query_len = query.shape[1]\n",
    "        key_len = key.shape[1]\n",
    "        value_len = value.shape[1] \n",
    "\n",
    "        query = query.reshape(B, query_len, self.n_heads, self.head_dim)\n",
    "        value = value.reshape(B, value_len, self.n_heads, self.head_dim)\n",
    "        key = key.reshape(B, key_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        query = self.queries(query)\n",
    "        value = self.values(value)\n",
    "        key = self.keys(key)\n",
    "\n",
    "        # compute energy \n",
    "        # B, query_len, n_heads, head_dim * B, key_len, n_heads, head_dim -> B, n_heads, query_len, key_len\n",
    "\n",
    "        energy = torch.einsum('bqnh,bknh->bnqk', [query, key])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('1e-20'))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_dim ** (0.5)), dim=3)\n",
    "\n",
    "        # compute output \n",
    "        # attention dim -> (N, heads, query_len, key_len)\n",
    "        # values dim -> (N, value_len, heads, head_dim)\n",
    "        # output dim -> (N, query_len, heads, head_dim)\n",
    "        \n",
    "        output = torch.einsum('nhqk,nvhd->nqhd', [attention, value]).reshape(B, query_len, self.n_heads * self.head_dim)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "#MultiHeadAttention()(torch.randn(5, 5, 256), torch.randn(5, 5, 256), torch.randn(5, 5, 256)).shape\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, kernel_size=9, embed_dim=256, forward_expansion=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv1d(embed_dim, embed_dim * forward_expansion, kernel_size, padding = (kernel_size - 1) // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(embed_dim * forward_expansion, embed_dim, kernel_size, padding = (kernel_size - 1) // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "#PositionWiseFeedForward()(torch.randn(5, 256, 100)).shape\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    encoding = torch.zeros((seq_len, d_model))\n",
    "\n",
    "    for k in range(seq_len):\n",
    "        for i in range(d_model // 2):\n",
    "            encoding[k, 2*i] = np.sin(k / (10000 ** ((2 * i) / d_model)))\n",
    "            encoding[k, 2*i + 1] = np.cos(k / (10000 ** ((2 * i) / d_model)))\n",
    "\n",
    "    return nn.Parameter(encoding)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "        self.feed_forward = PositionWiseFeedForward(embed_dim=embed_dim, forward_expansion=forward_expansion, dropout=dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        src_seq, src_mask = x\n",
    "        out, _ = self.attention(src_seq, src_seq, src_seq, src_mask)\n",
    "        out = self.dropout(self.layer_norm_1(out + src_seq))\n",
    "        out_2 = self.feed_forward(out.permute(0, 2, 1))\n",
    "        out_2 = self.dropout(self.layer_norm_2(out + out_2.permute(0, 2, 1)))\n",
    "        return out_2, src_mask\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, max_seq_len=200, src_vocab_size=64, n_layers=4, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = get_positional_encoding(max_seq_len, embed_dim)\n",
    "        self.phone_embedding = nn.Embedding(src_vocab_size, embed_dim, padding_idx=0)\n",
    "        self.encoder = nn.Sequential( *[ TransformerEncoderLayer(n_heads=n_heads, embed_dim=embed_dim, dropout=dropout, forward_expansion=forward_expansion) for _ in range(n_layers) ])\n",
    "\n",
    "    def forward(self, phonemes, mask=None):\n",
    "        phoneme_embeddings = self.phone_embedding(phonemes) # max_seq_len, embed_dim\n",
    "        model_in = self.pos_embedding + phoneme_embeddings\n",
    "        out = self.encoder((model_in, mask))\n",
    "        return out\n",
    "\n",
    "class VariancePredictor(nn.Module):\n",
    "    def __init__(self, embed_size=256, out_dim=1, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embed_size, embed_size, kernel_size, padding=1)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(embed_size, embed_size, kernel_size, padding=1)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x.permute(0, 2, 1)))\n",
    "        out = self.layer_norm_1(out.permute(0, 2, 1))\n",
    "        out = self.dropout_1(out)\n",
    "        out = F.relu(self.conv2(out.permute(0, 2, 1)))\n",
    "        out = self.layer_norm_2(out.permute(0, 2, 1))\n",
    "        out = self.dropout_2(out)\n",
    "        out = F.relu(self.fc_out(out))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class LengthRegulator(nn.Module):\n",
    "    def __init__(self, max_trg_len=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_trg_len = max_trg_len\n",
    "\n",
    "    def forward(self, encoder_output, variance):\n",
    "        B = encoder_output.shape[0]\n",
    "        mels = list()\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            expanded_seq = torch.concat([ encoder_output[b_idx,i,:].expand(v, -1) for i, v in enumerate(variance[b_idx, :]) ], dim=0)\n",
    "            seq_len = expanded_seq.shape[0]\n",
    "            pad_len = self.max_trg_len - seq_len\n",
    "\n",
    "            if pad_len < 0:\n",
    "                padded_seq = expanded_seq[:self.max_trg_len, :]\n",
    "            elif pad_len > 0:\n",
    "                padded_seq = F.pad( expanded_seq, (0, 0, 0, pad_len), \"constant\", -1 )\n",
    "\n",
    "            mels.append(padded_seq)\n",
    "\n",
    "\n",
    "        expanded_batch = torch.stack(mels, dim=0)\n",
    "        return expanded_batch\n",
    "    \n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "        self.feed_forward = PositionWiseFeedForward(embed_dim=embed_dim, forward_expansion=forward_expansion, dropout=dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trg_seq, trg_mask = x\n",
    "        out, _ = self.attention(trg_seq, trg_seq, trg_seq, trg_mask)\n",
    "        out = self.dropout(self.layer_norm_1(out + trg_seq))\n",
    "        out_2 = self.feed_forward(out.permute(0, 2, 1))\n",
    "        out_2 = self.dropout(self.layer_norm_2(out + out_2.permute(0, 2, 1)))\n",
    "        return out_2, trg_mask\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, max_seq_len=200, trg_vocab_size=64, n_layers=4, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = get_positional_encoding(max_seq_len, embed_dim)\n",
    "       # self.mel_embedding = nn.Embedding(trg_vocab_size, embed_dim)\n",
    "        self.decoder = nn.Sequential( *[ TransformerDecoderLayer(n_heads=n_heads, embed_dim=embed_dim, dropout=dropout, forward_expansion=forward_expansion) for _ in range(n_layers) ])\n",
    "\n",
    "    def forward(self, phonemes, trg_masks=None):\n",
    "        #mel_embeddings = self.mel_embedding(phonemes) # max_seq_len, embed_dim\n",
    "        model_in = self.pos_embedding + phonemes\n",
    "        out = self.decoder((model_in, trg_masks))\n",
    "        return out\n",
    "    \n",
    "class Hidden2Mel(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=80):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class PostNet(nn.Module):\n",
    "    def __init__(self, in_dim=80, postnet_dim=512, n_layers=5, kernel_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = list()\n",
    "        \n",
    "        layers.append(nn.Sequential(\n",
    "            nn.Conv1d(in_dim, postnet_dim, kernel_size=5, padding=(kernel_size - 1) // 2, bias=True),\n",
    "            nn.BatchNorm1d(postnet_dim)))\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(postnet_dim, postnet_dim, kernel_size=5, padding=(kernel_size - 1) // 2, bias=True),\n",
    "                    nn.BatchNorm1d(postnet_dim)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                    nn.Conv1d(postnet_dim, in_dim, kernel_size=5, padding=(kernel_size - 1) // 2, bias=True),\n",
    "                    nn.BatchNorm1d(in_dim)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x  + self.layers(x)\n",
    "    \n",
    "#PostNet()(torch.randn(5, 80, 100)).shape\n",
    "\n",
    "class FastSpeechLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "        self.decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "\n",
    "        self.duration_loss = nn.MSELoss()\n",
    "        self.mel_loss = nn.MSELoss()\n",
    "        self.h2m_loss = nn.MSELoss()\n",
    "\n",
    "    def _get_mask(self, seq_lens, max_seq_len):\n",
    "    # durations will be B, seq_len\n",
    "    # mels will be B, mel_channels, seq_len\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "        B = seq_lens.shape[0]\n",
    "        masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "        return masks.to(device)\n",
    "\n",
    "    def forward(self, src_seq_len, trg_seq_len, pred_durations, trg_durations, h2m_pred_mels, pred_mels, trg_mels):\n",
    "\n",
    "        duration_mask = self._get_mask(src_seq_len.reshape(-1, 1), self.encoder_max_seq_len)\n",
    "        mel_mask = self._get_mask(trg_seq_len.reshape(-1, 1), self.decoder_max_seq_len)\n",
    "\n",
    "        h2m_pred_mels = h2m_pred_mels.permute(0, 2, 1)\n",
    "        pred_mels = pred_mels\n",
    "        pred_durations = pred_durations.permute(0, 2, 1)\n",
    "        trg_mels = trg_mels.permute(0, 2, 1)\n",
    "        if duration_mask is not None:\n",
    "            pred_durations = pred_durations.masked_fill(duration_mask == 0, float('0'))\n",
    "\n",
    "        if mel_mask is not None:\n",
    "            pred_mels = pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "            h2m_pred_mels = h2m_pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "\n",
    "        duration_loss = self.duration_loss(pred_durations.squeeze(1), trg_durations.float())\n",
    "        h2m_loss = self.h2m_loss(h2m_pred_mels, trg_mels)\n",
    "        mel_loss = self.mel_loss(pred_mels, trg_mels)\n",
    "\n",
    "        return duration_loss, h2m_loss, mel_loss\n",
    "\n",
    "class FastSpeech(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__() \n",
    "\n",
    "        self.encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "        self.decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "\n",
    "        encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "        encoder_src_vocab_size = config['model']['encoder']['src_vocab_size']\n",
    "        encoder_n_layers = config['model']['encoder']['n_layers']\n",
    "        encoder_n_heads = config['model']['encoder']['n_heads']\n",
    "        encoder_embed_dim = config['model']['encoder']['embed_dim']\n",
    "        encoder_dropout = config['model']['encoder']['dropout']\n",
    "        encoder_forward_expansion = config['model']['encoder']['forward_expansion']\n",
    "\n",
    "        decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "        decoder_trg_vocab_size = config['model']['decoder']['trg_vocab_size']\n",
    "        decoder_n_layers = config['model']['decoder']['n_layers']\n",
    "        decoder_n_heads = config['model']['decoder']['n_heads']\n",
    "        decoder_embed_dim = config['model']['decoder']['embed_dim']\n",
    "        decoder_dropout = config['model']['decoder']['dropout']\n",
    "        decoder_forward_expansion = config['model']['decoder']['forward_expansion']\n",
    "\n",
    "        variance_predictor_embed_dim = config['model']['variance_predictor']['embed_dim']\n",
    "        variance_predictor_out_dim = config['model']['variance_predictor']['out_dim']\n",
    "        variance_predictor_kernel_size = config['model']['variance_predictor']['kernel_size']\n",
    "        variance_predictor_dropout = config['model']['variance_predictor']['dropout']\n",
    "\n",
    "        length_regulator_max_trg_len = config['model']['length_regulator']['max_trg_len']\n",
    "\n",
    "        hidden2mel_in_dim = config['model']['hidden_2_mel']['in_dim']\n",
    "        hidden2mel_out_dim = config['model']['hidden_2_mel']['out_dim']\n",
    "\n",
    "        postnet_in_dim = config['model']['postnet']['in_dim']\n",
    "        postnet_postnet_dim = config['model']['postnet']['postnet_dim']\n",
    "        postnet_n_layers = config['model']['postnet']['n_layers']\n",
    "        postnet_kernel_size = config['model']['postnet']['kernel_size']\n",
    "        \n",
    "        # encoder, variance adaptor, length regulator, decoder, hidden2mel, postnet\n",
    "\n",
    "        self.encoder = TransformerEncoder(max_seq_len=encoder_max_seq_len, src_vocab_size=encoder_src_vocab_size, n_layers=encoder_n_layers, n_heads=encoder_n_heads, embed_dim=encoder_embed_dim, dropout=encoder_dropout, forward_expansion=encoder_forward_expansion)\n",
    "        self.decoder = TransformerDecoder(max_seq_len=decoder_max_seq_len, trg_vocab_size=decoder_trg_vocab_size, n_layers=decoder_n_layers, n_heads=decoder_n_heads, embed_dim=decoder_embed_dim, dropout=decoder_dropout, forward_expansion=decoder_forward_expansion)\n",
    "        self.duration_predictor = VariancePredictor(embed_size=variance_predictor_embed_dim, out_dim=1, kernel_size=variance_predictor_kernel_size, dropout=variance_predictor_dropout)\n",
    "        self.length_regulator = LengthRegulator(max_trg_len=length_regulator_max_trg_len)\n",
    "        self.hidden2mel = Hidden2Mel(in_dim=hidden2mel_in_dim, out_dim=hidden2mel_out_dim)\n",
    "        self.postnet = PostNet(in_dim=postnet_in_dim, postnet_dim=postnet_postnet_dim, n_layers=postnet_n_layers, kernel_size=postnet_kernel_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        \n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _get_mask(self, seq_lens, max_seq_len):\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "        B = seq_lens.shape[0]\n",
    "        masks = torch.zeros(max_seq_len).repeat(B, 1, 1, 1)\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "        return masks.to(device)\n",
    "\n",
    "    def forward(self, src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations):\n",
    "        # the src seq is the list of phonemes, the trg_seq is the list of mel_specs\n",
    "        #enc_out = self.encoder(torch.ones(5, 100, 512, dtype = torch.int32))\n",
    "        \n",
    "        src_masks = self._get_mask(src_seq_len.reshape(-1, 1), self.encoder_max_seq_len)\n",
    "        trg_masks = self._get_mask(trg_seq_len.reshape(-1, 1), self.decoder_max_seq_len)\n",
    "        \n",
    "        enc_out, _ = self.encoder(src_seq, src_masks)\n",
    "        pred_durations = self.duration_predictor(enc_out)\n",
    "        adapted_enc_out = self.length_regulator(enc_out, trg_durations)\n",
    "        dec_out, _ = self.decoder(adapted_enc_out, trg_masks)\n",
    "        h2m_out = self.hidden2mel(dec_out)\n",
    "        pred_mel = self.postnet(h2m_out.permute(0, 2, 1))\n",
    "\n",
    "        return pred_mel, h2m_out, pred_durations \n",
    "    \n",
    "src_len, trg_len, dur, phone, mel = next(iter(train_loader))\n",
    "m  = FastSpeech(config=model_config)\n",
    "# pred_mel, h2m_out, pred_durations = m(phone, src_len, mel, trg_len, dur)\n",
    "# lo = FastSpeechLoss(config=model_config)\n",
    "# lo(src_len, trg_len, pred_durations, dur, h2m_out, pred_mel, mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 200]) torch.Size([4, 200]) torch.Size([4, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(29.8330, grad_fn=<MseLossBackward0>),\n",
       " tensor(4999.7119, grad_fn=<MseLossBackward0>),\n",
       " tensor(5000.2305, grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class FastSpeechLoss(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "#         self.decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "\n",
    "#         self.duration_loss = nn.MSELoss()\n",
    "#         self.mel_loss = nn.MSELoss()\n",
    "#         self.h2m_loss = nn.MSELoss()\n",
    "\n",
    "#     def _get_mask(self, seq_lens, max_seq_len):\n",
    "#     # durations will be B, seq_len\n",
    "#     # mels will be B, mel_channels, seq_len\n",
    "#     # seq_lens is B, seq_len\n",
    "#     # mask is B, 1, 1, max_seq_len\n",
    "#         B = seq_lens.shape[0]\n",
    "#         masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "#         for b_idx in range(B):\n",
    "#             masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "#         return masks\n",
    "\n",
    "#     def forward(self, src_seq_len, trg_seq_len, pred_durations, trg_durations, h2m_pred_mels, pred_mels, trg_mels):\n",
    "\n",
    "#         duration_mask = self._get_mask(src_seq_len.reshape(-1, 1), self.encoder_max_seq_len)\n",
    "#         mel_mask = self._get_mask(trg_seq_len.reshape(-1, 1), self.decoder_max_seq_len)\n",
    "\n",
    "#         h2m_pred_mels = h2m_pred_mels.permute(0, 2, 1)\n",
    "#         pred_mels = pred_mels\n",
    "#         pred_durations = pred_durations.permute(0, 2, 1)\n",
    "#         trg_mels = trg_mels.permute(0, 2, 1)\n",
    "#         if duration_mask is not None:\n",
    "#             pred_durations = pred_durations.masked_fill(duration_mask == 0, float('0'))\n",
    "\n",
    "#         if mel_mask is not None:\n",
    "#             pred_mels = pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "#             h2m_pred_mels = h2m_pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "\n",
    "#         duration_loss = self.duration_loss(pred_durations.squeeze(1), trg_durations)\n",
    "#         h2m_loss = self.h2m_loss(h2m_pred_mels, trg_mels)\n",
    "#         mel_loss = self.mel_loss(pred_mels, trg_mels)\n",
    "\n",
    "#         return duration_loss, h2m_loss, mel_loss\n",
    "\n",
    "# # src_seq_len, trg_seq_len, pred_durations, trg_durations, h2m_pred_mels, pred_mels, trg_mels)\n",
    "# lo = FastSpeechLoss(config=model_config)\n",
    "# lo(src_len, trg_len, pred_durations, dur, h2m_out, pred_mel, mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 200, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_durations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101,  66,  36,  61,  59,  63,  96,  73,  47,  86,  57,  36,  83,  71,\n",
       "         63,  82,  42,  48,  86,  71,  95,  47,  51,  76,  60,  51,  73,  46,\n",
       "         98,  34,  72,  67, 106,  96,  52,  97,  60,  30,  79,  99,  90,  98,\n",
       "         62,  66,  69, 104,  80,  37, 108,  60,  71,  51,  98,  18,  62,  42,\n",
       "         74,  67, 100,  39,  54,  34,  41,  91])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_len, trg_len, dur, phone, mel = next(iter(train_loader))\n",
    "m  = FastSpeech(config=model_config)\n",
    "src_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSpeechModule(pl.LightningModule):\n",
    "    def __init__(self, model_config_path):\n",
    "        super().__init__()\n",
    "\n",
    "        model_config = yaml.load(open(model_config_path, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = FastSpeech(config=model_config)\n",
    "        self.loss_module = FastSpeechLoss(config=model_config)\n",
    "        \n",
    "    def forward(self, src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations):\n",
    "        return self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[100, 150], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self,  batch, batch_idx):\n",
    "        #print(batch)'\n",
    "        #print('THIS IS A TRAINING STEP!!!!!!!!!!!!')\n",
    "        #src_len, trg_len, duration, phone_mapping, mel \n",
    "        src_seq_len, trg_seq_len, trg_durations, src_seq, trg_seq = batch\n",
    "        #src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations = batch \n",
    "        print(src_seq.shape, trg_seq.shape)\n",
    "        pred_mel, pred_h2m, pred_durations = self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "        dur_loss, h2m_loss, mel_loss = self.loss_module(src_seq_len, trg_seq_len, pred_durations, trg_durations, pred_h2m, pred_mel, trg_seq)\n",
    "\n",
    "        self.log('train_dur_error', dur_loss.item(), rank_zero_only=True)\n",
    "        self.log('train_h2m_error', h2m_loss.item(), rank_zero_only=True)\n",
    "        self.log('train_mel_error', mel_loss.item(), rank_zero_only=True)\n",
    "\n",
    "        return dur_loss + h2m_loss + mel_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src_seq_len, trg_seq_len, trg_durations, src_seq, trg_seq = batch\n",
    "        pred_mel, pred_h2m, pred_durations = self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "        dur_loss, h2m_loss, mel_loss = self.loss_module(src_seq_len, trg_seq_len, pred_durations, trg_durations, pred_h2m, pred_mel, trg_seq)\n",
    "\n",
    "        self.log('val_dur_error', dur_loss.item(), rank_zero_only=True)\n",
    "        self.log('val_h2m_error', h2m_loss.item(), rank_zero_only=True)\n",
    "        self.log('val_mel_error', mel_loss.item(), rank_zero_only=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src_seq_len, trg_seq_len, trg_durations, src_seq, trg_seq = batch\n",
    "        pred_mel, pred_h2m, pred_durations = self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "        dur_loss, h2m_loss, mel_loss = self.loss_module(src_seq_len, trg_seq_len, pred_durations, trg_durations, pred_h2m, pred_mel, trg_seq)\n",
    "\n",
    "        self.log('test_dur_error', dur_loss.item())\n",
    "        self.log('test_h2m_error', h2m_loss.item())\n",
    "        self.log('test_mel_error', mel_loss.item())\n",
    "\n",
    "#FastSpeechModule(model_config_path='../config/model.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, test_batch=True)\n",
    "val_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, test_batch=True)\n",
    "\n",
    "# set shuffle to false since we already shuffle when the data is split into train/test\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=len(train_dataset))\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | model       | FastSpeech     | 27.6 M\n",
      "1 | loss_module | FastSpeechLoss | 0     \n",
      "-----------------------------------------------\n",
      "27.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.6 M    Total params\n",
      "110.371   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1600: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 200]) torch.Size([16, 1000, 80])\n",
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, loss=5.82e+04, v_num=19]        torch.Size([16, 200]) torch.Size([16, 1000, 80])\n",
      "Epoch 1: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it, loss=5.81e+04, v_num=19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:13<00:00,  6.61s/it, loss=5.81e+04, v_num=19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:04<00:00,  4.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(FastSpeechModule(\n",
       "   (model): FastSpeech(\n",
       "     (encoder): TransformerEncoder(\n",
       "       (phone_embedding): Embedding(99, 256, padding_idx=0)\n",
       "       (encoder): Sequential(\n",
       "         (0): TransformerEncoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "         (1): TransformerEncoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "         (2): TransformerEncoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "         (3): TransformerEncoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (decoder): TransformerDecoder(\n",
       "       (decoder): Sequential(\n",
       "         (0): TransformerDecoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "         (1): TransformerDecoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "         (2): TransformerDecoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "         (3): TransformerDecoderLayer(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "             (fc_out): Linear(in_features=256, out_features=256, bias=False)\n",
       "           )\n",
       "           (feed_forward): PositionWiseFeedForward(\n",
       "             (layers): Sequential(\n",
       "               (0): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (1): ReLU()\n",
       "               (2): Conv1d(512, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "               (3): Dropout(p=0.4, inplace=False)\n",
       "               (4): ReLU()\n",
       "             )\n",
       "           )\n",
       "           (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.4, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (duration_predictor): VariancePredictor(\n",
       "       (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "       (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout_1): Dropout(p=0.2, inplace=False)\n",
       "       (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "       (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout_2): Dropout(p=0.2, inplace=False)\n",
       "       (fc_out): Linear(in_features=256, out_features=1, bias=True)\n",
       "     )\n",
       "     (length_regulator): LengthRegulator()\n",
       "     (hidden2mel): Hidden2Mel(\n",
       "       (layers): Sequential(\n",
       "         (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "         (1): ReLU()\n",
       "         (2): Linear(in_features=256, out_features=80, bias=True)\n",
       "         (3): ReLU()\n",
       "       )\n",
       "     )\n",
       "     (postnet): PostNet(\n",
       "       (layers): Sequential(\n",
       "         (0): Sequential(\n",
       "           (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "         (1): Sequential(\n",
       "           (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "         (2): Sequential(\n",
       "           (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "         (3): Sequential(\n",
       "           (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "         (4): Sequential(\n",
       "           (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "         (5): Sequential(\n",
       "           (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "         (6): Sequential(\n",
       "           (0): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "           (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (loss_module): FastSpeechLoss(\n",
       "     (duration_loss): MSELoss()\n",
       "     (mel_loss): MSELoss()\n",
       "     (h2m_loss): MSELoss()\n",
       "   )\n",
       " ),\n",
       " [{'test_dur_error': 17.3945255279541,\n",
       "   'test_h2m_error': 28993.279296875,\n",
       "   'test_mel_error': 47000.921875}])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "CHECKPOINT_PATH = 'B:\\Masters FS2\\checkpoints'\n",
    "\n",
    "def train_model(save_name='FastSpeech', **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",                     # We run on a GPU (if possible)\n",
    "                         devices=1,                                                                          # How many GPUs/CPUs we want to use (1 is enough for the notebooks)\n",
    "                         max_epochs=2,               \n",
    "                                                                               num_sanity_val_steps=0,                                                      # How many epochs to train for if no patience is set\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_mel_error\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "                                    LearningRateMonitor(\"epoch\")],                                           # Log learning rate every epoch\n",
    "                         enable_progress_bar=True,\n",
    "                         )                                                           # Set to False if you do not want a progress bar\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = FastSpeechModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = FastSpeechModule(model_config_path='../config/model.yaml')\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        #model = FastSpeechModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    return model, val_result\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 200])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask(seq_lens, max_seq_len):\n",
    "    # durations will be B, seq_len\n",
    "    # mels will be B, mel_channels, seq_len\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "    B = seq_lens.shape[0]\n",
    "    masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "    return masks\n",
    "\n",
    "mask = get_mask(src_len.reshape(-1, 1), 200)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask(seq_lens, max_seq_len):\n",
    "    # durations will be B, seq_len\n",
    "    # mels will be B, mel_channels, seq_len\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "    B = seq_lens.shape[0]\n",
    "    masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "    return masks\n",
    "\n",
    "mask = get_mask(trg_len.reshape(-1, 1), 1000)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0496e-05, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 5.2617e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 9.6685e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        ...,\n",
       "        [ 6.7222e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 3.4299e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 1.3991e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.permute(0, 2, 1).masked_fill(mask == 0, float(-6))[0][:, 723:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1000])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  6,  4,  3,  5,  6,  4,  7,  7,  8,  7,  4,  3,  4,  5,  8,  7,  5,\n",
       "          5,  8,  9,  5,  4,  4,  7,  4, 10,  8,  5,  3,  7, 11,  6,  6, 11, 29,\n",
       "          6,  2,  6,  3,  4, 10, 17,  8,  2,  3,  6, 10,  5, 13, 11,  4, 10,  8,\n",
       "          9,  7, 13,  6,  9,  3,  6,  3,  8,  7,  8,  5,  7,  5,  4,  5,  7,  5,\n",
       "          5,  7,  9, 10,  5,  5,  9,  5,  3,  2,  9, 11, 10,  4,  4, 10,  6, 11,\n",
       "          8,  3, 10, 13,  4,  8,  9,  3, 24, 13, 21, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur.reshape(4, 1, 200).masked_fill(mask == 0, float(-6))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LengthRegulator(nn.Module):\n",
    "    def __init__(self, max_trg_len=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_trg_len = max_trg_len\n",
    "\n",
    "    def forward(self, encoder_output, variance):\n",
    "        B = encoder_output.shape[0]\n",
    "        mels = list()\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            expanded_seq = torch.concat([ (i+1) * encoder_output[b_idx,i,:].expand(v, -1) for i, v in enumerate(variance[b_idx, :]) ], dim=0)\n",
    "            seq_len = expanded_seq.shape[0]\n",
    "            pad_len = self.max_trg_len - seq_len\n",
    "\n",
    "            if pad_len < 0:\n",
    "                padded_seq = expanded_seq[:self.max_trg_len, :]\n",
    "            elif pad_len > 0:\n",
    "                padded_seq = F.pad( expanded_seq, (0, 0, 0, pad_len), \"constant\", 0 )\n",
    "\n",
    "            mels.append(padded_seq)\n",
    "\n",
    "\n",
    "        expanded_batch = torch.stack(mels, dim=0)\n",
    "        return expanded_batch\n",
    "    \n",
    "LengthRegulator()(torch.ones(2, 3, 256), torch.tensor([ [2, 1, 0], [2, 1, 1] ]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-413f91836384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-155-413f91836384>\u001b[0m in \u001b[0;36mget_mask\u001b[1;34m(seq_lens, max_seq_len)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0midx\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "def get_mask(seq_lens, max_seq_len):\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "    B = seq_lens.shape[0]\n",
    "    masks = torch.zeros(max_seq_len).repeat(B, 1, 1, 1)\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "    return masks\n",
    "\n",
    "    \n",
    "\n",
    "m = get_mask(torch.tensor([[3], [5]]), 10)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = yaml.load(open('../config/model.yaml', 'r'), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B, n_heads, query_len, key_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 10, 256)\n",
    "mask = get_mask(torch.tensor([[0], [1]]), 10)\n",
    "out, attn = MultiHeadAttention()(x, x, x, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "for row in attn[0][0]:\n",
    "    print(row.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerDecoderLayer(nn.Module):\n",
    "#     def __init__(self, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.attention_1 = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "#         self.attention_2 = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "#         self.feed_forward = PositionWiseFeedForward(embed_dim=embed_dim, forward_expansion=forward_expansion, dropout=dropout)\n",
    "#         self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "#         self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "#         self.layer_norm_3 = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, enc_seq, trg_seq, src_mask=None, trg_mask=None):\n",
    "#         out, _ = self.attention_1(trg_seq, trg_seq, trg_seq, trg_mask)\n",
    "#         out = self.dropout(self.layer_norm_1(out + trg_seq))\n",
    "        \n",
    "#         out_2, _ = self.attention_2(trg_seq, enc_seq, enc_seq, src_mask)\n",
    "#         out_2 = self.dropout(self.layer_norm_2(out + out_2))\n",
    "\n",
    "#         out_3 = self.feed_forward(out_2.permute(0, 2, 1))\n",
    "#         out_3 = self.dropout(self.layer_norm_3(out_2 + out_3.permute(0, 2, 1)))\n",
    "#         return out_3\n",
    "\n",
    "\n",
    "# class TransformerDecoder(nn.Module):\n",
    "#     def __init__(self, max_seq_len=1000, trg_vocab_size=256, n_layers=4, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "#         self.pos_embedding = get_positional_encoding(max_seq_len, embed_dim)\n",
    "#         self.mel_embedding = nn.Embedding(trg_vocab_size, embed_dim) # B, max_enc_seq_len, enc_embed_dim -> B, max_trg_seq_len \n",
    "#         self.decoder = nn.Sequential( *[ TransformerDecoderLayer(n_heads=n_heads, embed_dim=embed_dim, dropout=dropout, forward_expansion=forward_expansion) for _ in range(n_layers) ])\n",
    "\n",
    "#     def forward(self, enc_out):\n",
    "#         mel_embeddings = self.mel_embedding(phonemes) # max_seq_len, embed_dim\n",
    "#         model_in = self.pos_embedding + phoneme_embeddings\n",
    "#         out = self.encoder(model_in)\n",
    "#         return out\n",
    "\n",
    "#TransformerDecoderLayer()(torch.randn(5, 100, 256), torch.randn(5, 1000, 256)).shape\n",
    "#LengthRegulator()(torch.randn(3, 5, 256), torch.tensor([ [1, 1, 2, 1, 1], [2, 1, 1, 1, 1], [1, 1, 1, 1, 6] ])).shape\n",
    "#VariancePredictor()(torch.randn(5, 100, 256))\n",
    "#TransformerEncoder()(torch.zeros(200, dtype=torch.int32).unsqueeze(0)).shape\n",
    "#TransformerEncoderLayer()(torch.randn(5, 100, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.tensor([[1,1,1,1], [2,2,2,2], [3,3,3,3], [4,4,4,4]])\n",
    "variance = torch.tensor([3, 1, 3, 1])\n",
    "t = torch.tensor([[1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4]])\n",
    "torch.concat([ t[:,i].expand(v, -1) for i, v in enumerate(variance) ], dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-400-c62da987becc>:3: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  torch.ones(256, dtype=torch.int32).unsqueeze(0).repeat((5, 1)) * torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        [3, 3, 3,  ..., 3, 3, 3],\n",
       "        [4, 4, 4,  ..., 4, 4, 4],\n",
       "        [5, 5, 5,  ..., 5, 5, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B, seq_len, embed_dim \n",
    "\n",
    "b = torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_gen(num_tokens, embed_dim=256):\n",
    "    return torch.range(1, num_tokens, dtype=torch.int32).unsqueeze(0).repeat(embed_dim, 1).T.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-410-d4ce199bcd6b>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  return torch.range(1, num_tokens, dtype=torch.int32).unsqueeze(0).repeat(embed_dim, 1).T.unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 256])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([tokens_gen(5), tokens_gen(3), tokens_gen(8)], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-410-d4ce199bcd6b>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  return torch.range(1, num_tokens, dtype=torch.int32).unsqueeze(0).repeat(embed_dim, 1).T.unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 256])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_gen(5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4]])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.tensor([[1,1,1,1], [2,2,2,2], [3,3,3,3], [4,4,4,4]])\n",
    "variance = torch.tensor([3, 1, 3, 1])\n",
    "t = torch.tensor([[1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4]])\n",
    "torch.concat([ t[:,i].expand(v, -1) for i, v in enumerate(variance) ], dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 256])\n",
      "torch.Size([3, 15, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-484-a6c7618c1e8d>:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  batch = torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T.unsqueeze(0).repeat(3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T.unsqueeze(0).repeat(3, 1, 1)\n",
    "v = torch.tensor([ [1, 1, 2, 1, 1], [2, 1, 1, 1, 1], [1, 1, 1, 1, 6] ])\n",
    "max_trg_len = 15\n",
    "\n",
    "def expand_seqs(encoder_output, variances):\n",
    "    B = encoder_output.shape[0]\n",
    "    mels = list()\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        expanded_seq = torch.concat([ encoder_output[b_idx,i,:].expand(v, -1) for i, v in enumerate(variances[b_idx, :]) ], dim=0)\n",
    "        seq_len = expanded_seq.shape[0]\n",
    "        pad_len = max_trg_len - seq_len\n",
    "\n",
    "        if pad_len < 0:\n",
    "            padded_seq = expanded_seq[:max_trg_len, :]\n",
    "        elif pad_len > 0:\n",
    "            padded_seq = F.pad( expanded_seq, (0, 0, 0, pad_len), \"constant\", -1 )\n",
    "\n",
    "        mels.append(padded_seq)\n",
    "\n",
    "\n",
    "    expanded_batch = torch.stack(mels, dim=0)\n",
    "    return expanded_batch\n",
    "\n",
    "\n",
    "print(batch.shape)\n",
    "print(expand_seqs(batch, v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        [3, 3, 3,  ..., 3, 3, 3],\n",
       "        [4, 4, 4,  ..., 4, 4, 4],\n",
       "        [5, 5, 5,  ..., 5, 5, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 256])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad( batch[0], (0, 0, 0, 2), \"constant\", -1 ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0].expand(2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 256])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([batch[0], batch[0][0].expand(2, -1)]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
