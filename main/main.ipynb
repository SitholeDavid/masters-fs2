{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR KAGGLE\n",
    "\n",
    "# input_dir = 'kaggle/input'\n",
    "# working_dir = 'kaggle/working'\n",
    "\n",
    "# !git clone https://github.com/SitholeDavid/masters-fs2 working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "from text import _clean_text\n",
    "from text import _symbol_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import tgt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchaudio as TA\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_config = yaml.load(open('../config/preprocessing.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "model_config = yaml.load(open('../config/model.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "raw_path = preprocessing_config['path']['raw_path']\n",
    "input_dir = preprocessing_config['path']['input_dir']\n",
    "\n",
    "sampling_rate = preprocessing_config['audio']['sampling_rate']\n",
    "hop_length = preprocessing_config['audio']['hop_length']\n",
    "win_length = preprocessing_config['audio']['win_length']\n",
    "n_fft = preprocessing_config['audio']['n_fft']\n",
    "n_mels = preprocessing_config['audio']['n_mel_channels']\n",
    "fmin = preprocessing_config['audio']['mel_fmin']\n",
    "fmax = preprocessing_config['audio']['mel_fmax']\n",
    "cleaners = preprocessing_config['text']['text_cleaners']\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(raw_path, 'metadata.csv'), sep='|', header=None)\n",
    "metadata.columns = ['file', 'text', 'text_']\n",
    "metadata.drop(['text_'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input dir and copy files over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../data/raw_data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = './../kaggle_data/raw_data'\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    os.makedirs(input_dir)\n",
    "\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../data/raw_data'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "silent_phones = [\"sil\", \"sp\", \"spn\"]\n",
    "mel_spec_transform = MelSpectrogram(sample_rate=sampling_rate, n_fft=n_fft, win_length=win_length, hop_length=hop_length, f_min=fmin, f_max=fmax, n_mels=n_mels)\n",
    "\n",
    "for line in tqdm(metadata.iterrows()):\n",
    "    file = line[1]['file']\n",
    "    text = line[1]['text']\n",
    "    clean_text = _clean_text(text, cleaner_names=cleaners)\n",
    "    \n",
    "    textgrid = tgt.io.read_textgrid(os.path.join(raw_path, 'TextGrid', 'LJSpeech', f'{file}.TextGrid'))\n",
    "    textgrid = textgrid.get_tier_by_name('phones')\n",
    "\n",
    "    phonemes = []\n",
    "    durations = []\n",
    "\n",
    "    start_time = 0.0\n",
    "    end_time = 0.0\n",
    "    end_idx = 0\n",
    "    \n",
    "    for tier in textgrid._objects:\n",
    "\n",
    "        # trim the initial silent phonemes\n",
    "        if tier.text in silent_phones and len(durations) == 0:\n",
    "            continue\n",
    "\n",
    "        # record the start time\n",
    "        if len(phonemes) == 0:\n",
    "            start_time = tier.start_time\n",
    "\n",
    "        phonemes.append(tier.text)\n",
    "        durations.append(round( tier.end_time * sampling_rate / hop_length ) - round( tier.start_time * sampling_rate / hop_length ))\n",
    "\n",
    "        # trim the last silent phonemes\n",
    "        if tier.text not in silent_phones:\n",
    "            end_time = tier.end_time\n",
    "            end_idx = len(phonemes)\n",
    "\n",
    "       \n",
    " \n",
    "    phonemes = phonemes[:end_idx]\n",
    "    durations = durations[:end_idx]\n",
    "    phoneme_seq = \"{\" + \" \".join(phonemes) + \"}\"\n",
    "\n",
    "    wav_path = os.path.join(raw_path, 'wavs', f'{file}.wav')\n",
    "    audio, _ = librosa.load(wav_path, sr=sampling_rate)\n",
    "    audio = torch.clip(torch.FloatTensor(audio).unsqueeze(0), -1, 1)\n",
    "\n",
    "\n",
    "    # compute the fundamental frequency\n",
    "    pitch, energy = librosa.core.pitch.piptrack(\n",
    "        y=audio.numpy(),\n",
    "        sr=sampling_rate,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    pitch = pitch[: sum(durations)]\n",
    "    \n",
    "    if np.sum(pitch != 0) <= 1:\n",
    "        continue\n",
    "\n",
    "    ref_pitch = np.load('B:/FastSpeech2/preprocessed_data/LJSpeech/pitch/LJSpeech-pitch-LJ001-0001.npy')\n",
    "\n",
    "    # averaging\n",
    "    nonzero_ids = np.where(pitch != 0)[0]\n",
    "    \n",
    "    interp_fn = interp1d(\n",
    "        nonzero_ids,\n",
    "        pitch[nonzero_ids],\n",
    "        fill_value=(pitch[nonzero_ids[0]], pitch[nonzero_ids[-1]]),\n",
    "        bounds_error=False,\n",
    "    )\n",
    "    pitch = interp_fn(np.arange(0, len(pitch)))\n",
    "\n",
    "    # Phoneme-level average\n",
    "    pos = 0\n",
    "    for i, d in enumerate(durations):\n",
    "        if d > 0:\n",
    "            pitch[i] = np.mean(pitch[pos : pos + d])\n",
    "        else:\n",
    "            pitch[i] = 0\n",
    "        pos += d\n",
    "    pitch = pitch[: len(durations)]\n",
    "\n",
    "    print(ref_pitch.max())\n",
    "    break\n",
    "\n",
    "    # compute the mel-spectogram\n",
    "    mel = mel_spec_transform(audio).squeeze(0)[:, :sum(durations)]\n",
    "\n",
    "    assert mel.shape[-1] == np.sum(durations), f\"Number of frames not equal for file {line[0]}\"\n",
    "\n",
    "    sample_input_dir = os.path.join(input_dir, \"LJSpeech\", file)\n",
    "    audio_file_name = os.path.join(sample_input_dir, f'{file}.wav')\n",
    "    mel_file_name = os.path.join(sample_input_dir, f'{file}-mel.npy')\n",
    "    duration_file_name = os.path.join(sample_input_dir, f'{file}-duration.npy')\n",
    "    text_file_name = os.path.join(sample_input_dir, f'{file}.csv')\n",
    "\n",
    "    if not os.path.exists(sample_input_dir):\n",
    "        os.makedirs(sample_input_dir)\n",
    "\n",
    "    wavfile.write(audio_file_name, sampling_rate, audio.reshape(-1, 1).numpy())\n",
    "    np.save(mel_file_name, mel.T)\n",
    "    np.save(duration_file_name, durations)\n",
    "    np.save(mel_file_name, mel.T)\n",
    "\n",
    "    with open(text_file_name, 'w') as f:\n",
    "        f.write(f'{file[0]} | {clean_text} | {phoneme_seq}')\n",
    "\n",
    "    sample_temp_dir = os.path.join(temp_dir, \"LJSpeech\", file)\n",
    "    audio_file_name = os.path.join(sample_temp_dir, f'{file}.wav')\n",
    "    mel_file_name = os.path.join(sample_temp_dir, f'{file}-mel.npy')\n",
    "    duration_file_name = os.path.join(sample_temp_dir, f'{file}-duration.npy')\n",
    "    text_file_name = os.path.join(sample_temp_dir, f'{file}.csv')\n",
    "\n",
    "    if not os.path.exists(sample_temp_dir):\n",
    "        os.makedirs(sample_temp_dir)\n",
    "\n",
    "    #wavfile.write(audio_file_name, sampling_rate, audio.reshape(-1, 1).numpy())\n",
    "    np.save(mel_file_name, mel.T)\n",
    "    np.save(duration_file_name, durations)\n",
    "    np.save(mel_file_name, mel.T)\n",
    "\n",
    "    with open(text_file_name, 'w') as f:\n",
    "        f.write(f'{file[0]} | {clean_text} | {phoneme_seq}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input test begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dir = 'B:/Masters FS2/data/raw_data/LJSpeech'\n",
    "phoneme_counter = Counter()\n",
    "valid_text_counter = Counter()\n",
    "\n",
    "for folder in os.listdir(txt_dir):\n",
    "    file = pd.read_csv(os.path.join(txt_dir, folder, f'{folder}.csv'), header=None, sep='|')\n",
    "    phonemes = file[2].values[0].strip()[1:-1].split(' ')\n",
    "    phoneme_counter.update(phonemes)\n",
    "    valid_text_counter.update([all(symbol in _symbol_to_id for symbol in phonemes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({True: 13084, False: 16})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_text_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'P': 19825,\n",
       "         'R': 38632,\n",
       "         'IH1': 27617,\n",
       "         'N': 67837,\n",
       "         'T': 64882,\n",
       "         'IH0': 28808,\n",
       "         'NG': 7141,\n",
       "         'sp': 29781,\n",
       "         'DH': 28620,\n",
       "         'IY0': 19128,\n",
       "         'OW1': 7928,\n",
       "         'L': 32387,\n",
       "         'S': 42909,\n",
       "         'EH1': 23349,\n",
       "         'W': 20053,\n",
       "         'CH': 4727,\n",
       "         'IY1': 13590,\n",
       "         'AA1': 11293,\n",
       "         'AE1': 22941,\n",
       "         'Z': 27344,\n",
       "         'AH0': 75646,\n",
       "         'K': 27453,\n",
       "         'ER1': 5215,\n",
       "         'D': 42761,\n",
       "         'F': 16670,\n",
       "         'ER0': 19892,\n",
       "         'AH1': 18293,\n",
       "         'M': 23035,\n",
       "         'AO1': 12432,\n",
       "         'EH2': 1992,\n",
       "         'B': 15245,\n",
       "         'SH': 7873,\n",
       "         'V': 19420,\n",
       "         'AO2': 663,\n",
       "         'AY0': 398,\n",
       "         'UH1': 2685,\n",
       "         'G': 5877,\n",
       "         'EY1': 12982,\n",
       "         'AH2': 341,\n",
       "         'AY1': 10350,\n",
       "         'UW1': 9543,\n",
       "         'TH': 4237,\n",
       "         'JH': 4781,\n",
       "         'AW1': 3871,\n",
       "         'HH': 14088,\n",
       "         'Y': 4863,\n",
       "         'AY2': 1107,\n",
       "         'AA2': 808,\n",
       "         'EH0': 598,\n",
       "         'OW0': 1396,\n",
       "         'spn': 1822,\n",
       "         'AE2': 1134,\n",
       "         'IH2': 1323,\n",
       "         'IY2': 467,\n",
       "         'EY2': 1368,\n",
       "         'UW0': 1152,\n",
       "         'AE0': 398,\n",
       "         'AA0': 273,\n",
       "         'AW2': 313,\n",
       "         'ZH': 609,\n",
       "         'ER2': 150,\n",
       "         'EY0': 498,\n",
       "         'OW2': 587,\n",
       "         'OY1': 803,\n",
       "         'AO0': 1377,\n",
       "         'UW2': 449,\n",
       "         'AW0': 36,\n",
       "         'UH2': 130,\n",
       "         'UH0': 37,\n",
       "         'isease,': 1,\n",
       "         'cold,': 1,\n",
       "         'famine,': 1,\n",
       "         'nakedness,': 1,\n",
       "         'and': 18,\n",
       "         'contagious': 1,\n",
       "         'polluted': 1,\n",
       "         'air': 1,\n",
       "         'are': 6,\n",
       "         'not': 2,\n",
       "         'lawful': 1,\n",
       "         'punishments': 1,\n",
       "         'in': 10,\n",
       "         'the': 24,\n",
       "         'hands': 1,\n",
       "         'of': 11,\n",
       "         'civil': 1,\n",
       "         'magistrates': 1,\n",
       "         'll': 1,\n",
       "         'measures': 2,\n",
       "         'practices': 1,\n",
       "         'prison': 3,\n",
       "         'which': 4,\n",
       "         'may': 1,\n",
       "         'injure': 1,\n",
       "         'him': 2,\n",
       "         'any': 1,\n",
       "         'way': 1,\n",
       "         'illegal': 1,\n",
       "         't': 1,\n",
       "         'is': 2,\n",
       "         'obvious': 1,\n",
       "         'that': 1,\n",
       "         'reformation': 1,\n",
       "         'must': 3,\n",
       "         'be': 6,\n",
       "         'materially': 1,\n",
       "         'impeded,': 1,\n",
       "         'some': 1,\n",
       "         'cases': 1,\n",
       "         'utterly': 1,\n",
       "         'defeated,': 1,\n",
       "         'when': 1,\n",
       "         'prisoners': 1,\n",
       "         'defectively': 1,\n",
       "         'classed': 1,\n",
       "         'here': 2,\n",
       "         'should': 1,\n",
       "         'no': 3,\n",
       "         'tea': 1,\n",
       "         'sugar,': 1,\n",
       "         'assemblage': 1,\n",
       "         'female': 1,\n",
       "         'felons': 1,\n",
       "         'around': 1,\n",
       "         'washing-tub': 1,\n",
       "         'n': 1,\n",
       "         'prisons,': 1,\n",
       "         'really': 1,\n",
       "         'meant': 1,\n",
       "         'to': 8,\n",
       "         'keep': 1,\n",
       "         'multitude': 1,\n",
       "         'order,': 1,\n",
       "         'a': 3,\n",
       "         'terror': 1,\n",
       "         'evil-doers,': 1,\n",
       "         'there': 1,\n",
       "         'sharings': 1,\n",
       "         'profits': 1,\n",
       "         'xpedient': 1,\n",
       "         'introduce': 1,\n",
       "         'such': 1,\n",
       "         'arrangements': 1,\n",
       "         'as': 2,\n",
       "         'shall': 1,\n",
       "         'only': 1,\n",
       "         'provide': 1,\n",
       "         'for': 2,\n",
       "         'safe': 1,\n",
       "         'custody': 1,\n",
       "         'o': 2,\n",
       "         'inquire': 1,\n",
       "         'into': 1,\n",
       "         'report': 1,\n",
       "         'upon': 1,\n",
       "         'several': 1,\n",
       "         'jails': 1,\n",
       "         'houses': 1,\n",
       "         'correction': 1,\n",
       "         'counties,': 1,\n",
       "         'cities,': 1,\n",
       "         'corporate': 1,\n",
       "         'towns': 1,\n",
       "         'within': 2,\n",
       "         'england': 1,\n",
       "         'wale': 1,\n",
       "         'he': 2,\n",
       "         'comparatively': 1,\n",
       "         'innocent': 1,\n",
       "         'seduced,': 1,\n",
       "         'unwary': 1,\n",
       "         'entrapped': 1,\n",
       "         'great': 1,\n",
       "         'authority': 1,\n",
       "         'exercised': 1,\n",
       "         'by': 1,\n",
       "         'OY2': 44,\n",
       "         'was': 2,\n",
       "         'disturbance': 1,\n",
       "         'transport': 1,\n",
       "         'yard': 1,\n",
       "         'yesterday': 1,\n",
       "         'evening,': 1,\n",
       "         'police': 1,\n",
       "         'were': 2,\n",
       "         'called': 1,\n",
       "         'hat': 1,\n",
       "         'state': 1,\n",
       "         'discipline': 1,\n",
       "         'place': 1,\n",
       "         'filled': 1,\n",
       "         'with': 1,\n",
       "         'characters': 1,\n",
       "         'so': 2,\n",
       "         'various': 1,\n",
       "         'assembled': 1,\n",
       "         'there,': 1,\n",
       "         'where': 1,\n",
       "         'tried': 1,\n",
       "         'untried,': 1,\n",
       "         'sick': 1,\n",
       "         'healthy': 1,\n",
       "         's': 1,\n",
       "         'permitted': 1,\n",
       "         'purchase': 1,\n",
       "         'whatever': 1,\n",
       "         'his': 2,\n",
       "         'own': 1,\n",
       "         'means': 2,\n",
       "         'or': 2,\n",
       "         'friends': 1,\n",
       "         'out': 1,\n",
       "         'can': 1,\n",
       "         'afford': 1,\n",
       "         'hich': 1,\n",
       "         'has': 1,\n",
       "         'been': 1,\n",
       "         'demonstrably': 1,\n",
       "         'proved': 1,\n",
       "         'fruitful': 1,\n",
       "         'source': 1,\n",
       "         'all': 1,\n",
       "         'abuses': 1,\n",
       "         'irregularities': 1,\n",
       "         'have': 2,\n",
       "         'long': 2,\n",
       "         'disgraced': 1,\n",
       "         'newgate': 1,\n",
       "         'prominent': 1,\n",
       "         'evils': 2,\n",
       "         'this': 1,\n",
       "         '(newgate)': 1,\n",
       "         '--': 1,\n",
       "         'alterations': 1,\n",
       "         'made': 1,\n",
       "         'last': 1,\n",
       "         'four': 1,\n",
       "         'years': 1,\n",
       "         'failed': 1,\n",
       "         'remov': 1,\n",
       "         'ill': 1,\n",
       "         'three': 1,\n",
       "         \"o'clock\": 1,\n",
       "         'it': 1,\n",
       "         'one': 1,\n",
       "         'revelry': 1,\n",
       "         'songs': 1,\n",
       "         'laughter,': 1,\n",
       "         'shouting,': 1,\n",
       "         'often': 1,\n",
       "         'quarreling,': 1,\n",
       "         'though': 1,\n",
       "         'OY0': 4,\n",
       "         'or,': 1,\n",
       "         'without': 1,\n",
       "         'waiting': 1,\n",
       "         'dispute': 1,\n",
       "         'wisdom': 1,\n",
       "         'making': 1,\n",
       "         'dwarfed': 1,\n",
       "         'useless': 1,\n",
       "         'structures': 1,\n",
       "         'merely': 1,\n",
       "         'whimsical': 1,\n",
       "         'motive': 1,\n",
       "         'assigned': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phoneme_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(symbol in _symbol_to_id for symbol in ['AE', 'AA0', 'BOZA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_': 0,\n",
       " '-': 1,\n",
       " '!': 2,\n",
       " \"'\": 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " ',': 6,\n",
       " '.': 7,\n",
       " ':': 8,\n",
       " ';': 9,\n",
       " '?': 10,\n",
       " ' ': 11,\n",
       " 'AA': 12,\n",
       " 'AA0': 13,\n",
       " 'AA1': 14,\n",
       " 'AA2': 15,\n",
       " 'AE': 16,\n",
       " 'AE0': 17,\n",
       " 'AE1': 18,\n",
       " 'AE2': 19,\n",
       " 'AH': 20,\n",
       " 'AH0': 21,\n",
       " 'AH1': 22,\n",
       " 'AH2': 23,\n",
       " 'AO': 24,\n",
       " 'AO0': 25,\n",
       " 'AO1': 26,\n",
       " 'AO2': 27,\n",
       " 'AW': 28,\n",
       " 'AW0': 29,\n",
       " 'AW1': 30,\n",
       " 'AW2': 31,\n",
       " 'AY': 32,\n",
       " 'AY0': 33,\n",
       " 'AY1': 34,\n",
       " 'AY2': 35,\n",
       " 'B': 36,\n",
       " 'CH': 37,\n",
       " 'D': 38,\n",
       " 'DH': 39,\n",
       " 'EH': 40,\n",
       " 'EH0': 41,\n",
       " 'EH1': 42,\n",
       " 'EH2': 43,\n",
       " 'ER': 44,\n",
       " 'ER0': 45,\n",
       " 'ER1': 46,\n",
       " 'ER2': 47,\n",
       " 'EY': 48,\n",
       " 'EY0': 49,\n",
       " 'EY1': 50,\n",
       " 'EY2': 51,\n",
       " 'F': 52,\n",
       " 'G': 53,\n",
       " 'HH': 54,\n",
       " 'IH': 55,\n",
       " 'IH0': 56,\n",
       " 'IH1': 57,\n",
       " 'IH2': 58,\n",
       " 'IY': 59,\n",
       " 'IY0': 60,\n",
       " 'IY1': 61,\n",
       " 'IY2': 62,\n",
       " 'JH': 63,\n",
       " 'K': 64,\n",
       " 'L': 65,\n",
       " 'M': 66,\n",
       " 'N': 67,\n",
       " 'NG': 68,\n",
       " 'OW': 69,\n",
       " 'OW0': 70,\n",
       " 'OW1': 71,\n",
       " 'OW2': 72,\n",
       " 'OY': 73,\n",
       " 'OY0': 74,\n",
       " 'OY1': 75,\n",
       " 'OY2': 76,\n",
       " 'P': 77,\n",
       " 'R': 78,\n",
       " 'S': 79,\n",
       " 'SH': 80,\n",
       " 'T': 81,\n",
       " 'TH': 82,\n",
       " 'UH': 83,\n",
       " 'UH0': 84,\n",
       " 'UH1': 85,\n",
       " 'UH2': 86,\n",
       " 'UW': 87,\n",
       " 'UW0': 88,\n",
       " 'UW1': 89,\n",
       " 'UW2': 90,\n",
       " 'V': 91,\n",
       " 'W': 92,\n",
       " 'Y': 93,\n",
       " 'Z': 94,\n",
       " 'ZH': 95,\n",
       " 'sp': 96,\n",
       " 'spn': 97,\n",
       " 'sil': 98}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_symbol_to_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input test end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LJSpeechDataset(Dataset):\n",
    "    def __init__(self,  input_dir, input_file, max_src_len, max_trg_len, split='train', test_batch=False):\n",
    "        metadata = pd.read_csv(input_file, sep='|', header=None)\n",
    "        metadata.columns = ['file', 'text', 'text_']\n",
    "\n",
    "        self.input_dir = input_dir\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_trg_len = max_trg_len\n",
    "        \n",
    "        \n",
    "        file_names = metadata['file'].to_numpy()\n",
    "\n",
    "        # only consider valid phoneme sequences\n",
    "        valid_file_names = np.array([])\n",
    "\n",
    "        for file_name in file_names:\n",
    "            phonemes = pd.read_csv(os.path.join(self.input_dir, file_name, f'{file_name}.csv'), sep='|', header=None).iloc[0][2].strip()[1:-1].split(' ')\n",
    "            \n",
    "            if all(symbol in _symbol_to_id for symbol in phonemes):\n",
    "                valid_file_names = np.append(valid_file_names, file_name)\n",
    "\n",
    "        file_names = valid_file_names\n",
    "\n",
    "        if test_batch:\n",
    "            self.file_names = file_names[:16]\n",
    "            return\n",
    "\n",
    "        x_train, x_test = train_test_split(file_names, test_size=0.2, random_state=42, shuffle=True)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.file_names = x_train\n",
    "        else:\n",
    "            self.file_names = x_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        mel = np.load(os.path.join(self.input_dir, file_name, f'{file_name}-mel.npy'))\n",
    "        duration = np.load(os.path.join(self.input_dir, file_name, f'{file_name}-duration.npy'))\n",
    "        phones = pd.read_csv(os.path.join(self.input_dir, file_name, f'{file_name}.csv'), sep='|', header=None).iloc[0][2].strip()[1:-1].split(' ')\n",
    "        phone_mapping = torch.tensor([ _symbol_to_id[symbol] for symbol in phones ])\n",
    "        \n",
    "\n",
    "        src_len = torch.tensor(len(phones))\n",
    "        trg_len = torch.tensor(mel.shape[0])\n",
    "\n",
    "        phoneme_pad_length = self.max_src_len - src_len\n",
    "        mel_pad_length = self.max_trg_len - trg_len\n",
    "        \n",
    "        phone_mapping = F.pad(phone_mapping, (0, phoneme_pad_length), mode='constant', value=0)\n",
    "        duration = F.pad( torch.tensor(duration), (0, phoneme_pad_length), mode='constant', value=0)\n",
    "        mel = F.pad(torch.tensor(mel), (0, 0, 0, mel_pad_length), mode='constant', value=0)\n",
    "        \n",
    "        return  src_len, trg_len, duration, phone_mapping, mel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, split='train')\n",
    "val_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, split='test')\n",
    "\n",
    "# set shuffle to false since we already shuffle when the data is split into train/test\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=4)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads=8, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.embed_dim = embed_dim \n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        assert self.head_dim * self.n_heads == self.embed_dim, \"The embedding dimension must be divisible by the number of heads\"\n",
    "\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(self.head_dim * self.n_heads, self.embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query_dim -> B, num_phonemes, embed_dim -> B, num_phonemes, n_heads, head_dim\n",
    "        B = query.shape[0]\n",
    "        query_len = query.shape[1]\n",
    "        key_len = key.shape[1]\n",
    "        value_len = value.shape[1] \n",
    "\n",
    "        query = query.reshape(B, query_len, self.n_heads, self.head_dim)\n",
    "        value = value.reshape(B, value_len, self.n_heads, self.head_dim)\n",
    "        key = key.reshape(B, key_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        query = self.queries(query)\n",
    "        value = self.values(value)\n",
    "        key = self.keys(key)\n",
    "\n",
    "        # compute energy \n",
    "        # B, query_len, n_heads, head_dim * B, key_len, n_heads, head_dim -> B, n_heads, query_len, key_len\n",
    "\n",
    "        energy = torch.einsum('bqnh,bknh->bnqk', [query, key])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('1e-20'))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_dim ** (0.5)), dim=3)\n",
    "\n",
    "        # compute output \n",
    "        # attention dim -> (N, heads, query_len, key_len)\n",
    "        # values dim -> (N, value_len, heads, head_dim)\n",
    "        # output dim -> (N, query_len, heads, head_dim)\n",
    "        \n",
    "        output = torch.einsum('nhqk,nvhd->nqhd', [attention, value]).reshape(B, query_len, self.n_heads * self.head_dim)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "#MultiHeadAttention()(torch.randn(5, 5, 256), torch.randn(5, 5, 256), torch.randn(5, 5, 256)).shape\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, kernel_size=9, embed_dim=256, forward_expansion=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Conv1d(embed_dim, embed_dim * forward_expansion, kernel_size, padding = (kernel_size - 1) // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(embed_dim * forward_expansion, embed_dim, kernel_size, padding = (kernel_size - 1) // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "#PositionWiseFeedForward()(torch.randn(5, 256, 100)).shape\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    encoding = torch.zeros((seq_len, d_model))\n",
    "\n",
    "    for k in range(seq_len):\n",
    "        for i in range(d_model // 2):\n",
    "            encoding[k, 2*i] = np.sin(k / (10000 ** ((2 * i) / d_model)))\n",
    "            encoding[k, 2*i + 1] = np.cos(k / (10000 ** ((2 * i) / d_model)))\n",
    "\n",
    "    return nn.Parameter(encoding)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "        self.feed_forward = PositionWiseFeedForward(embed_dim=embed_dim, forward_expansion=forward_expansion, dropout=dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        src_seq, src_mask = x\n",
    "        out, _ = self.attention(src_seq, src_seq, src_seq, src_mask)\n",
    "        out = self.dropout(self.layer_norm_1(out + src_seq))\n",
    "        out_2 = self.feed_forward(out.permute(0, 2, 1))\n",
    "        out_2 = self.dropout(self.layer_norm_2(out + out_2.permute(0, 2, 1)))\n",
    "        return out_2, src_mask\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, max_seq_len=200, src_vocab_size=64, n_layers=4, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = get_positional_encoding(max_seq_len, embed_dim)\n",
    "        self.phone_embedding = nn.Embedding(src_vocab_size, embed_dim, padding_idx=0)\n",
    "        self.encoder = nn.Sequential( *[ TransformerEncoderLayer(n_heads=n_heads, embed_dim=embed_dim, dropout=dropout, forward_expansion=forward_expansion) for _ in range(n_layers) ])\n",
    "\n",
    "    def forward(self, phonemes, mask=None):\n",
    "        phoneme_embeddings = self.phone_embedding(phonemes) # max_seq_len, embed_dim\n",
    "        model_in = self.pos_embedding + phoneme_embeddings\n",
    "        out = self.encoder((model_in, mask))\n",
    "        return out\n",
    "\n",
    "class VariancePredictor(nn.Module):\n",
    "    def __init__(self, embed_size=256, out_dim=1, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embed_size, embed_size, kernel_size, padding=1)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(embed_size, embed_size, kernel_size, padding=1)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x.permute(0, 2, 1)))\n",
    "        out = self.layer_norm_1(out.permute(0, 2, 1))\n",
    "        out = self.dropout_1(out)\n",
    "        out = F.relu(self.conv2(out.permute(0, 2, 1)))\n",
    "        out = self.layer_norm_2(out.permute(0, 2, 1))\n",
    "        out = self.dropout_2(out)\n",
    "        out = F.relu(self.fc_out(out))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class LengthRegulator(nn.Module):\n",
    "    def __init__(self, max_trg_len=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_trg_len = max_trg_len\n",
    "\n",
    "    def forward(self, encoder_output, variance):\n",
    "        B = encoder_output.shape[0]\n",
    "        mels = list()\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            expanded_seq = torch.concat([ encoder_output[b_idx,i,:].expand(v, -1) for i, v in enumerate(variance[b_idx, :]) ], dim=0)\n",
    "            seq_len = expanded_seq.shape[0]\n",
    "            pad_len = self.max_trg_len - seq_len\n",
    "\n",
    "            if pad_len < 0:\n",
    "                padded_seq = expanded_seq[:self.max_trg_len, :]\n",
    "            elif pad_len > 0:\n",
    "                padded_seq = F.pad( expanded_seq, (0, 0, 0, pad_len), \"constant\", -1 )\n",
    "\n",
    "            mels.append(padded_seq)\n",
    "\n",
    "\n",
    "        expanded_batch = torch.stack(mels, dim=0)\n",
    "        return expanded_batch\n",
    "    \n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "        self.feed_forward = PositionWiseFeedForward(embed_dim=embed_dim, forward_expansion=forward_expansion, dropout=dropout)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trg_seq, trg_mask = x\n",
    "        out, _ = self.attention(trg_seq, trg_seq, trg_seq, trg_mask)\n",
    "        out = self.dropout(self.layer_norm_1(out + trg_seq))\n",
    "        out_2 = self.feed_forward(out.permute(0, 2, 1))\n",
    "        out_2 = self.dropout(self.layer_norm_2(out + out_2.permute(0, 2, 1)))\n",
    "        return out_2, trg_mask\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, max_seq_len=200, trg_vocab_size=64, n_layers=4, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = get_positional_encoding(max_seq_len, embed_dim)\n",
    "       # self.mel_embedding = nn.Embedding(trg_vocab_size, embed_dim)\n",
    "        self.decoder = nn.Sequential( *[ TransformerDecoderLayer(n_heads=n_heads, embed_dim=embed_dim, dropout=dropout, forward_expansion=forward_expansion) for _ in range(n_layers) ])\n",
    "\n",
    "    def forward(self, phonemes, trg_masks=None):\n",
    "        #mel_embeddings = self.mel_embedding(phonemes) # max_seq_len, embed_dim\n",
    "        model_in = self.pos_embedding + phonemes\n",
    "        out = self.decoder((model_in, trg_masks))\n",
    "        return out\n",
    "    \n",
    "class Hidden2Mel(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=80):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "         )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class PostNet(nn.Module):\n",
    "    def __init__(self, in_dim=80, postnet_dim=512, n_layers=5, kernel_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = list()\n",
    "        \n",
    "        layers.append(nn.Sequential(\n",
    "            nn.Conv1d(in_dim, postnet_dim, kernel_size=5, padding=(kernel_size - 1) // 2, bias=True),\n",
    "            nn.BatchNorm1d(postnet_dim)))\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(postnet_dim, postnet_dim, kernel_size=5, padding=(kernel_size - 1) // 2, bias=True),\n",
    "                    nn.BatchNorm1d(postnet_dim)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                    nn.Conv1d(postnet_dim, in_dim, kernel_size=5, padding=(kernel_size - 1) // 2, bias=True),\n",
    "                    nn.BatchNorm1d(in_dim)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x  + self.layers(x)\n",
    "    \n",
    "#PostNet()(torch.randn(5, 80, 100)).shape\n",
    "\n",
    "class FastSpeechLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "        self.decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "\n",
    "        self.duration_loss = nn.MSELoss()\n",
    "        self.mel_loss = nn.MSELoss()\n",
    "        self.h2m_loss = nn.MSELoss()\n",
    "\n",
    "    def _get_mask(self, seq_lens, max_seq_len):\n",
    "    # durations will be B, seq_len\n",
    "    # mels will be B, mel_channels, seq_len\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "        B = seq_lens.shape[0]\n",
    "        masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "        return masks.to(device)\n",
    "\n",
    "    def forward(self, src_seq_len, trg_seq_len, pred_durations, trg_durations, h2m_pred_mels, pred_mels, trg_mels):\n",
    "\n",
    "        duration_mask = self._get_mask(src_seq_len.reshape(-1, 1), self.encoder_max_seq_len)\n",
    "        mel_mask = self._get_mask(trg_seq_len.reshape(-1, 1), self.decoder_max_seq_len)\n",
    "\n",
    "        h2m_pred_mels = h2m_pred_mels.permute(0, 2, 1)\n",
    "        pred_mels = pred_mels\n",
    "        pred_durations = pred_durations.permute(0, 2, 1)\n",
    "        trg_mels = trg_mels.permute(0, 2, 1)\n",
    "        if duration_mask is not None:\n",
    "            pred_durations = pred_durations.masked_fill(duration_mask == 0, float('0'))\n",
    "\n",
    "        if mel_mask is not None:\n",
    "            pred_mels = pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "            h2m_pred_mels = h2m_pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "\n",
    "        duration_loss = self.duration_loss(pred_durations.squeeze(1), trg_durations.float())\n",
    "        h2m_loss = self.h2m_loss(h2m_pred_mels, trg_mels)\n",
    "        mel_loss = self.mel_loss(pred_mels, trg_mels)\n",
    "\n",
    "        return duration_loss, h2m_loss, mel_loss\n",
    "\n",
    "class FastSpeech(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__() \n",
    "\n",
    "        self.encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "        self.decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "\n",
    "        encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "        encoder_src_vocab_size = config['model']['encoder']['src_vocab_size']\n",
    "        encoder_n_layers = config['model']['encoder']['n_layers']\n",
    "        encoder_n_heads = config['model']['encoder']['n_heads']\n",
    "        encoder_embed_dim = config['model']['encoder']['embed_dim']\n",
    "        encoder_dropout = config['model']['encoder']['dropout']\n",
    "        encoder_forward_expansion = config['model']['encoder']['forward_expansion']\n",
    "\n",
    "        decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "        decoder_trg_vocab_size = config['model']['decoder']['trg_vocab_size']\n",
    "        decoder_n_layers = config['model']['decoder']['n_layers']\n",
    "        decoder_n_heads = config['model']['decoder']['n_heads']\n",
    "        decoder_embed_dim = config['model']['decoder']['embed_dim']\n",
    "        decoder_dropout = config['model']['decoder']['dropout']\n",
    "        decoder_forward_expansion = config['model']['decoder']['forward_expansion']\n",
    "\n",
    "        variance_predictor_embed_dim = config['model']['variance_predictor']['embed_dim']\n",
    "        variance_predictor_out_dim = config['model']['variance_predictor']['out_dim']\n",
    "        variance_predictor_kernel_size = config['model']['variance_predictor']['kernel_size']\n",
    "        variance_predictor_dropout = config['model']['variance_predictor']['dropout']\n",
    "\n",
    "        length_regulator_max_trg_len = config['model']['length_regulator']['max_trg_len']\n",
    "\n",
    "        hidden2mel_in_dim = config['model']['hidden_2_mel']['in_dim']\n",
    "        hidden2mel_out_dim = config['model']['hidden_2_mel']['out_dim']\n",
    "\n",
    "        postnet_in_dim = config['model']['postnet']['in_dim']\n",
    "        postnet_postnet_dim = config['model']['postnet']['postnet_dim']\n",
    "        postnet_n_layers = config['model']['postnet']['n_layers']\n",
    "        postnet_kernel_size = config['model']['postnet']['kernel_size']\n",
    "        \n",
    "        # encoder, variance adaptor, length regulator, decoder, hidden2mel, postnet\n",
    "\n",
    "        self.encoder = TransformerEncoder(max_seq_len=encoder_max_seq_len, src_vocab_size=encoder_src_vocab_size, n_layers=encoder_n_layers, n_heads=encoder_n_heads, embed_dim=encoder_embed_dim, dropout=encoder_dropout, forward_expansion=encoder_forward_expansion)\n",
    "        self.decoder = TransformerDecoder(max_seq_len=decoder_max_seq_len, trg_vocab_size=decoder_trg_vocab_size, n_layers=decoder_n_layers, n_heads=decoder_n_heads, embed_dim=decoder_embed_dim, dropout=decoder_dropout, forward_expansion=decoder_forward_expansion)\n",
    "        self.duration_predictor = VariancePredictor(embed_size=variance_predictor_embed_dim, out_dim=1, kernel_size=variance_predictor_kernel_size, dropout=variance_predictor_dropout)\n",
    "        self.length_regulator = LengthRegulator(max_trg_len=length_regulator_max_trg_len)\n",
    "        self.hidden2mel = Hidden2Mel(in_dim=hidden2mel_in_dim, out_dim=hidden2mel_out_dim)\n",
    "        self.postnet = PostNet(in_dim=postnet_in_dim, postnet_dim=postnet_postnet_dim, n_layers=postnet_n_layers, kernel_size=postnet_kernel_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        \n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _get_mask(self, seq_lens, max_seq_len):\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "        B = seq_lens.shape[0]\n",
    "        masks = torch.zeros(max_seq_len).repeat(B, 1, 1, 1)\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "        return masks.to(device)\n",
    "\n",
    "    def forward(self, src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations):\n",
    "        # the src seq is the list of phonemes, the trg_seq is the list of mel_specs\n",
    "        #enc_out = self.encoder(torch.ones(5, 100, 512, dtype = torch.int32))\n",
    "        \n",
    "        src_masks = self._get_mask(src_seq_len.reshape(-1, 1), self.encoder_max_seq_len)\n",
    "        trg_masks = self._get_mask(trg_seq_len.reshape(-1, 1), self.decoder_max_seq_len)\n",
    "        \n",
    "        enc_out, _ = self.encoder(src_seq, src_masks)\n",
    "        pred_durations = self.duration_predictor(enc_out)\n",
    "        adapted_enc_out = self.length_regulator(enc_out, trg_durations)\n",
    "        dec_out, _ = self.decoder(adapted_enc_out, trg_masks)\n",
    "        h2m_out = self.hidden2mel(dec_out)\n",
    "        pred_mel = self.postnet(h2m_out.permute(0, 2, 1))\n",
    "\n",
    "        return pred_mel, h2m_out, pred_durations \n",
    "    \n",
    "src_len, trg_len, dur, phone, mel = next(iter(train_loader))\n",
    "m  = FastSpeech(config=model_config)\n",
    "# pred_mel, h2m_out, pred_durations = m(phone, src_len, mel, trg_len, dur)\n",
    "# lo = FastSpeechLoss(config=model_config)\n",
    "# lo(src_len, trg_len, pred_durations, dur, h2m_out, pred_mel, mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FastSpeechLoss(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.encoder_max_seq_len = config['model']['encoder']['max_seq_len']\n",
    "#         self.decoder_max_seq_len = config['model']['decoder']['max_seq_len']\n",
    "#         self.duration_loss = nn.MSELoss()\n",
    "#         self.mel_loss = nn.MSELoss()\n",
    "#         self.h2m_loss = nn.MSELoss()\n",
    "\n",
    "#     def _get_mask(self, seq_lens, max_seq_len):\n",
    "#     # durations will be B, seq_len\n",
    "#     # mels will be B, mel_channels, seq_len\n",
    "#     # seq_lens is B, seq_len\n",
    "#     # mask is B, 1, 1, max_seq_len\n",
    "#         B = seq_lens.shape[0]\n",
    "#         masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "#         for b_idx in range(B):\n",
    "#             masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "#         return masks\n",
    "\n",
    "#     def forward(self, src_seq_len, trg_seq_len, pred_durations, trg_durations, h2m_pred_mels, pred_mels, trg_mels):\n",
    "\n",
    "#         duration_mask = self._get_mask(src_seq_len.reshape(-1, 1), self.encoder_max_seq_len)\n",
    "#         mel_mask = self._get_mask(trg_seq_len.reshape(-1, 1), self.decoder_max_seq_len)\n",
    "\n",
    "#         h2m_pred_mels = h2m_pred_mels.permute(0, 2, 1)\n",
    "#         pred_mels = pred_mels\n",
    "#         pred_durations = pred_durations.permute(0, 2, 1)\n",
    "#         trg_mels = trg_mels.permute(0, 2, 1)\n",
    "#         if duration_mask is not None:\n",
    "#             pred_durations = pred_durations.masked_fill(duration_mask == 0, float('0'))\n",
    "\n",
    "#         if mel_mask is not None:\n",
    "#             pred_mels = pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "#             h2m_pred_mels = h2m_pred_mels.masked_fill(mel_mask == 0, float('0'))\n",
    "\n",
    "#         duration_loss = self.duration_loss(pred_durations.squeeze(1), trg_durations)\n",
    "#         h2m_loss = self.h2m_loss(h2m_pred_mels, trg_mels)\n",
    "#         mel_loss = self.mel_loss(pred_mels, trg_mels)\n",
    "\n",
    "#         return duration_loss, h2m_loss, mel_loss\n",
    "\n",
    "# # src_seq_len, trg_seq_len, pred_durations, trg_durations, h2m_pred_mels, pred_mels, trg_mels)\n",
    "# lo = FastSpeechLoss(config=model_config)\n",
    "# lo(src_len, trg_len, pred_durations, dur, h2m_out, pred_mel, mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 200, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_durations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101,  66,  36,  61,  59,  63,  96,  73,  47,  86,  57,  36,  83,  71,\n",
       "         63,  82,  42,  48,  86,  71,  95,  47,  51,  76,  60,  51,  73,  46,\n",
       "         98,  34,  72,  67, 106,  96,  52,  97,  60,  30,  79,  99,  90,  98,\n",
       "         62,  66,  69, 104,  80,  37, 108,  60,  71,  51,  98,  18,  62,  42,\n",
       "         74,  67, 100,  39,  54,  34,  41,  91])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_len, trg_len, dur, phone, mel = next(iter(train_loader))\n",
    "m  = FastSpeech(config=model_config)\n",
    "src_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Union\n",
    "\n",
    "\n",
    "from pytorch_lightning.core.optimizer import LightningOptimizer\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class FastSpeechModule(pl.LightningModule):\n",
    "    def __init__(self, model_config_path):\n",
    "        super().__init__()\n",
    "\n",
    "        model_config = yaml.load(open(model_config_path, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = FastSpeech(config=model_config)\n",
    "        self.loss_module = FastSpeechLoss(config=model_config)\n",
    "        \n",
    "    def forward(self, src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations):\n",
    "        return self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[100, 150], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
    "        n_warmup_steps = 500\n",
    "        anneal_steps = [3000, 4000, 5000]\n",
    "        anneal_rate = 0.3\n",
    "        current_step = self.trainer.global_step\n",
    "        init_lr = np.power(256, -0.5)\n",
    "\n",
    "        lr = np.min(\n",
    "            [\n",
    "                np.power(current_step, -0.5),\n",
    "                np.power(n_warmup_steps, -1.5) * current_step,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for s in anneal_steps:\n",
    "            if current_step > s:\n",
    "                lr = lr * anneal_rate\n",
    "        \n",
    "        lr = init_lr * lr\n",
    "\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "    \n",
    "    def training_step(self,  batch, batch_idx):\n",
    "        #print(batch)'\n",
    "        #print('THIS IS A TRAINING STEP!!!!!!!!!!!!')\n",
    "        #src_len, trg_len, duration, phone_mapping, mel \n",
    "        src_seq_len, trg_seq_len, trg_durations, src_seq, trg_seq = batch\n",
    "        #src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations = batch \n",
    "        pred_mel, pred_h2m, pred_durations = self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "        dur_loss, h2m_loss, mel_loss = self.loss_module(src_seq_len, trg_seq_len, pred_durations, trg_durations, pred_h2m, pred_mel, trg_seq)\n",
    "\n",
    "        self.logger.log_metrics('train_dur_error', dur_loss.item(), rank_zero_only=True)\n",
    "        self.log('train_h2m_error', h2m_loss.item(), rank_zero_only=True)\n",
    "        self.log('train_mel_error', mel_loss.item(), rank_zero_only=True)\n",
    "\n",
    "        return dur_loss + h2m_loss + mel_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src_seq_len, trg_seq_len, trg_durations, src_seq, trg_seq = batch\n",
    "        pred_mel, pred_h2m, pred_durations = self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "        dur_loss, h2m_loss, mel_loss = self.loss_module(src_seq_len, trg_seq_len, pred_durations, trg_durations, pred_h2m, pred_mel, trg_seq)\n",
    "\n",
    "        self.log('val_dur_error', dur_loss.item(), rank_zero_only=True)\n",
    "        self.log('val_h2m_error', h2m_loss.item(), rank_zero_only=True)\n",
    "        self.log('val_mel_error', mel_loss.item(), rank_zero_only=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src_seq_len, trg_seq_len, trg_durations, src_seq, trg_seq = batch\n",
    "        pred_mel, pred_h2m, pred_durations = self.model(src_seq, src_seq_len, trg_seq, trg_seq_len, trg_durations)\n",
    "        dur_loss, h2m_loss, mel_loss = self.loss_module(src_seq_len, trg_seq_len, pred_durations, trg_durations, pred_h2m, pred_mel, trg_seq)\n",
    "\n",
    "        self.log('test_dur_error', dur_loss.item())\n",
    "        self.log('test_h2m_error', h2m_loss.item())\n",
    "        self.log('test_mel_error', mel_loss.item())\n",
    "\n",
    "#FastSpeechModule(model_config_path='../config/model.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, test_batch=True)\n",
    "val_dataset = LJSpeechDataset(os.path.join(input_dir, \"LJSpeech\"), os.path.join(raw_path, 'metadata.csv'), max_src_len=200, max_trg_len=1000, test_batch=True)\n",
    "\n",
    "# set shuffle to false since we already shuffle when the data is split into train/test\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=len(train_dataset))\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"FastSpeech2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | model       | FastSpeech     | 27.6 M\n",
      "1 | loss_module | FastSpeechLoss | 0     \n",
      "-----------------------------------------------\n",
      "27.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.6 M    Total params\n",
      "110.371   Total estimated model params size (MB)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "log_metrics() got an unexpected keyword argument 'rank_zero_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-b5470e7a2ade>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-b5470e7a2ade>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(save_name, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# To be reproducable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFastSpeechModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../config/model.yaml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;31m#model = FastSpeechModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"`Trainer.fit()` requires a `LightningModule`, got: {model.__class__.__qualname__}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[0;32m    609\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         )\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         )\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1182\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_to_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_training_epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_training_batch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch_idx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             )\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim_progress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[1;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;31m# the `batch_idx` is optional with inter-batch parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch_idx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    368\u001b[0m             )\n\u001b[0;32m    369\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"using_native_amp\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMixedPrecisionPlugin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         self.trainer._call_lightning_module_hook(\n\u001b[0m\u001b[0;32m    371\u001b[0m             \u001b[1;34m\"optimizer_step\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[1;31m# restore current_fx when nested context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\core\\module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1743\u001b[0m         \"\"\"\n\u001b[1;32m-> 1744\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mstep_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;31m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         return self.precision_plugin.optimizer_step(\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         )\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;34m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_track_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"pl.Trainer\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mconsistent\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPrecisionPlugin\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubclasses\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcannot\u001b[0m \u001b[1;32mpass\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdirectly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mclosure_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mclosure_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36mclosure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mstep_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m_training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \"\"\"\n\u001b[0;32m    418\u001b[0m         \u001b[1;31m# manually capture logged metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mtraining_step_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_step\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m         \u001b[1;31m# restore current_fx when nested context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainingStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-50b0fb8929ad>\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdur_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2m_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_durations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_durations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_h2m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_mel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_dur_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdur_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_h2m_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2m_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_mel_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightning_utilities\\core\\rank_zero.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The `rank_zero_only.rank` needs to be set before use\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: log_metrics() got an unexpected keyword argument 'rank_zero_only'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "CHECKPOINT_PATH = 'B:\\Masters FS2\\checkpoints'\n",
    "\n",
    "def train_model(save_name='FastSpeech', **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(\n",
    "                        logger=logger,\n",
    "                        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",                     # We run on a GPU (if possible)\n",
    "                         devices=1,                                                                          # How many GPUs/CPUs we want to use (1 is enough for the notebooks)\n",
    "                         max_epochs=500,    \n",
    "                         gradient_clip_val=2.0,\n",
    "                         log_every_n_steps=1,          \n",
    "                                                                            num_sanity_val_steps=0,                                                      # How many epochs to train for if no patience is set\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_mel_error\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "                                    LearningRateMonitor(\"epoch\")],                                           # Log learning rate every epoch\n",
    "                         enable_progress_bar=True,\n",
    "                         )                                                           # Set to False if you do not want a progress bar\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = FastSpeechModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = FastSpeechModule(model_config_path='../config/model.yaml')\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        #model = FastSpeechModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    return model, val_result\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 200])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask(seq_lens, max_seq_len):\n",
    "    # durations will be B, seq_len\n",
    "    # mels will be B, mel_channels, seq_len\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "    B = seq_lens.shape[0]\n",
    "    masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "    return masks\n",
    "\n",
    "mask = get_mask(src_len.reshape(-1, 1), 200)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask(seq_lens, max_seq_len):\n",
    "    # durations will be B, seq_len\n",
    "    # mels will be B, mel_channels, seq_len\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "    B = seq_lens.shape[0]\n",
    "    masks = torch.zeros(max_seq_len).repeat(B, 1, 1)\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "    return masks\n",
    "\n",
    "mask = get_mask(trg_len.reshape(-1, 1), 1000)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0496e-05, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 5.2617e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 9.6685e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        ...,\n",
       "        [ 6.7222e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 3.4299e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00],\n",
       "        [ 1.3991e-04, -6.0000e+00, -6.0000e+00,  ..., -6.0000e+00,\n",
       "         -6.0000e+00, -6.0000e+00]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.permute(0, 2, 1).masked_fill(mask == 0, float(-6))[0][:, 723:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1000])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  6,  4,  3,  5,  6,  4,  7,  7,  8,  7,  4,  3,  4,  5,  8,  7,  5,\n",
       "          5,  8,  9,  5,  4,  4,  7,  4, 10,  8,  5,  3,  7, 11,  6,  6, 11, 29,\n",
       "          6,  2,  6,  3,  4, 10, 17,  8,  2,  3,  6, 10,  5, 13, 11,  4, 10,  8,\n",
       "          9,  7, 13,  6,  9,  3,  6,  3,  8,  7,  8,  5,  7,  5,  4,  5,  7,  5,\n",
       "          5,  7,  9, 10,  5,  5,  9,  5,  3,  2,  9, 11, 10,  4,  4, 10,  6, 11,\n",
       "          8,  3, 10, 13,  4,  8,  9,  3, 24, 13, 21, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6, -6,\n",
       "         -6, -6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur.reshape(4, 1, 200).masked_fill(mask == 0, float(-6))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LengthRegulator(nn.Module):\n",
    "    def __init__(self, max_trg_len=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_trg_len = max_trg_len\n",
    "\n",
    "    def forward(self, encoder_output, variance):\n",
    "        B = encoder_output.shape[0]\n",
    "        mels = list()\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            expanded_seq = torch.concat([ (i+1) * encoder_output[b_idx,i,:].expand(v, -1) for i, v in enumerate(variance[b_idx, :]) ], dim=0)\n",
    "            seq_len = expanded_seq.shape[0]\n",
    "            pad_len = self.max_trg_len - seq_len\n",
    "\n",
    "            if pad_len < 0:\n",
    "                padded_seq = expanded_seq[:self.max_trg_len, :]\n",
    "            elif pad_len > 0:\n",
    "                padded_seq = F.pad( expanded_seq, (0, 0, 0, pad_len), \"constant\", 0 )\n",
    "\n",
    "            mels.append(padded_seq)\n",
    "\n",
    "\n",
    "        expanded_batch = torch.stack(mels, dim=0)\n",
    "        return expanded_batch\n",
    "    \n",
    "LengthRegulator()(torch.ones(2, 3, 256), torch.tensor([ [2, 1, 0], [2, 1, 1] ]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-413f91836384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-155-413f91836384>\u001b[0m in \u001b[0;36mget_mask\u001b[1;34m(seq_lens, max_seq_len)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0midx\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "def get_mask(seq_lens, max_seq_len):\n",
    "    # seq_lens is B, seq_len\n",
    "    # mask is B, 1, 1, max_seq_len\n",
    "    B = seq_lens.shape[0]\n",
    "    masks = torch.zeros(max_seq_len).repeat(B, 1, 1, 1)\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        masks[b_idx, :] = torch.tensor( [ [ seq_len > idx  for idx in torch.arange(0, max_seq_len) ]  for seq_len in seq_lens[b_idx, :] ])\n",
    "\n",
    "    return masks\n",
    "\n",
    "    \n",
    "\n",
    "m = get_mask(torch.tensor([[3], [5]]), 10)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = yaml.load(open('../config/model.yaml', 'r'), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B, n_heads, query_len, key_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 10, 256)\n",
    "mask = get_mask(torch.tensor([[0], [1]]), 10)\n",
    "out, attn = MultiHeadAttention()(x, x, x, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "for row in attn[0][0]:\n",
    "    print(row.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerDecoderLayer(nn.Module):\n",
    "#     def __init__(self, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.attention_1 = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "#         self.attention_2 = MultiHeadAttention(n_heads=n_heads, embed_dim=embed_dim)\n",
    "#         self.feed_forward = PositionWiseFeedForward(embed_dim=embed_dim, forward_expansion=forward_expansion, dropout=dropout)\n",
    "#         self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "#         self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "#         self.layer_norm_3 = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, enc_seq, trg_seq, src_mask=None, trg_mask=None):\n",
    "#         out, _ = self.attention_1(trg_seq, trg_seq, trg_seq, trg_mask)\n",
    "#         out = self.dropout(self.layer_norm_1(out + trg_seq))\n",
    "        \n",
    "#         out_2, _ = self.attention_2(trg_seq, enc_seq, enc_seq, src_mask)\n",
    "#         out_2 = self.dropout(self.layer_norm_2(out + out_2))\n",
    "\n",
    "#         out_3 = self.feed_forward(out_2.permute(0, 2, 1))\n",
    "#         out_3 = self.dropout(self.layer_norm_3(out_2 + out_3.permute(0, 2, 1)))\n",
    "#         return out_3\n",
    "\n",
    "\n",
    "# class TransformerDecoder(nn.Module):\n",
    "#     def __init__(self, max_seq_len=1000, trg_vocab_size=256, n_layers=4, n_heads=8, embed_dim=256, dropout=0.5, forward_expansion=2):\n",
    "#         self.pos_embedding = get_positional_encoding(max_seq_len, embed_dim)\n",
    "#         self.mel_embedding = nn.Embedding(trg_vocab_size, embed_dim) # B, max_enc_seq_len, enc_embed_dim -> B, max_trg_seq_len \n",
    "#         self.decoder = nn.Sequential( *[ TransformerDecoderLayer(n_heads=n_heads, embed_dim=embed_dim, dropout=dropout, forward_expansion=forward_expansion) for _ in range(n_layers) ])\n",
    "\n",
    "#     def forward(self, enc_out):\n",
    "#         mel_embeddings = self.mel_embedding(phonemes) # max_seq_len, embed_dim\n",
    "#         model_in = self.pos_embedding + phoneme_embeddings\n",
    "#         out = self.encoder(model_in)\n",
    "#         return out\n",
    "\n",
    "#TransformerDecoderLayer()(torch.randn(5, 100, 256), torch.randn(5, 1000, 256)).shape\n",
    "#LengthRegulator()(torch.randn(3, 5, 256), torch.tensor([ [1, 1, 2, 1, 1], [2, 1, 1, 1, 1], [1, 1, 1, 1, 6] ])).shape\n",
    "#VariancePredictor()(torch.randn(5, 100, 256))\n",
    "#TransformerEncoder()(torch.zeros(200, dtype=torch.int32).unsqueeze(0)).shape\n",
    "#TransformerEncoderLayer()(torch.randn(5, 100, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.tensor([[1,1,1,1], [2,2,2,2], [3,3,3,3], [4,4,4,4]])\n",
    "variance = torch.tensor([3, 1, 3, 1])\n",
    "t = torch.tensor([[1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4]])\n",
    "torch.concat([ t[:,i].expand(v, -1) for i, v in enumerate(variance) ], dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-400-c62da987becc>:3: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  torch.ones(256, dtype=torch.int32).unsqueeze(0).repeat((5, 1)) * torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        [3, 3, 3,  ..., 3, 3, 3],\n",
       "        [4, 4, 4,  ..., 4, 4, 4],\n",
       "        [5, 5, 5,  ..., 5, 5, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B, seq_len, embed_dim \n",
    "\n",
    "b = torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_gen(num_tokens, embed_dim=256):\n",
    "    return torch.range(1, num_tokens, dtype=torch.int32).unsqueeze(0).repeat(embed_dim, 1).T.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-410-d4ce199bcd6b>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  return torch.range(1, num_tokens, dtype=torch.int32).unsqueeze(0).repeat(embed_dim, 1).T.unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 256])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([tokens_gen(5), tokens_gen(3), tokens_gen(8)], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-410-d4ce199bcd6b>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  return torch.range(1, num_tokens, dtype=torch.int32).unsqueeze(0).repeat(embed_dim, 1).T.unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 256])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_gen(5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4],\n",
       "        [1, 1, 1, 2, 3, 3, 3, 4]])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.tensor([[1,1,1,1], [2,2,2,2], [3,3,3,3], [4,4,4,4]])\n",
    "variance = torch.tensor([3, 1, 3, 1])\n",
    "t = torch.tensor([[1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4], [1,2,3,4]])\n",
    "torch.concat([ t[:,i].expand(v, -1) for i, v in enumerate(variance) ], dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 256])\n",
      "torch.Size([3, 15, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-484-a6c7618c1e8d>:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  batch = torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T.unsqueeze(0).repeat(3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.range(1, 5, dtype=torch.int32).unsqueeze(0).repeat(256, 1).T.unsqueeze(0).repeat(3, 1, 1)\n",
    "v = torch.tensor([ [1, 1, 2, 1, 1], [2, 1, 1, 1, 1], [1, 1, 1, 1, 6] ])\n",
    "max_trg_len = 15\n",
    "\n",
    "def expand_seqs(encoder_output, variances):\n",
    "    B = encoder_output.shape[0]\n",
    "    mels = list()\n",
    "\n",
    "    for b_idx in range(B):\n",
    "        expanded_seq = torch.concat([ encoder_output[b_idx,i,:].expand(v, -1) for i, v in enumerate(variances[b_idx, :]) ], dim=0)\n",
    "        seq_len = expanded_seq.shape[0]\n",
    "        pad_len = max_trg_len - seq_len\n",
    "\n",
    "        if pad_len < 0:\n",
    "            padded_seq = expanded_seq[:max_trg_len, :]\n",
    "        elif pad_len > 0:\n",
    "            padded_seq = F.pad( expanded_seq, (0, 0, 0, pad_len), \"constant\", -1 )\n",
    "\n",
    "        mels.append(padded_seq)\n",
    "\n",
    "\n",
    "    expanded_batch = torch.stack(mels, dim=0)\n",
    "    return expanded_batch\n",
    "\n",
    "\n",
    "print(batch.shape)\n",
    "print(expand_seqs(batch, v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        [3, 3, 3,  ..., 3, 3, 3],\n",
       "        [4, 4, 4,  ..., 4, 4, 4],\n",
       "        [5, 5, 5,  ..., 5, 5, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 256])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad( batch[0], (0, 0, 0, 2), \"constant\", -1 ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0].expand(2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 256])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([batch[0], batch[0][0].expand(2, -1)]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
